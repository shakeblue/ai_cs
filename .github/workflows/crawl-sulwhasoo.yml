name: Crawl Sulwhasoo (설화수)

on:
  # Schedule: Run every 6 hours (high-value brand)
  schedule:
    - cron: '0 */6 * * *'  # At minute 0 past every 6th hour (UTC)

  # Manual trigger via GitHub Actions UI
  workflow_dispatch:
    inputs:
      limit:
        description: 'Maximum number of broadcasts to crawl'
        required: false
        default: '50'
      concurrency:
        description: 'Number of parallel crawlers (5-10 recommended)'
        required: false
        default: '10'
      verbose:
        description: 'Enable verbose logging'
        required: false
        type: boolean
        default: true

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Optimized crawler completes much faster but keeping safe margin

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'crawler/cj/requirements.txt'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            chromium-browser \
            chromium-chromedriver \
            fonts-noto-cjk

      - name: Install Python dependencies
        working-directory: crawler/cj
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright browsers
        run: |
          playwright install chromium
          playwright install-deps chromium

      - name: Run optimized standalone crawler for Sulwhasoo
        working-directory: crawler/cj
        run: |
          LIMIT="${{ github.event.inputs.limit || '50' }}"
          CONCURRENCY="${{ github.event.inputs.concurrency || '10' }}"
          VERBOSE_FLAG=""
          if [ "${{ github.event.inputs.verbose }}" = "true" ] || [ -z "${{ github.event.inputs.verbose }}" ]; then
            VERBOSE_FLAG="--verbose"
          fi

          python standalone_crawler_optimized.py \
            --brand-name "Sulwhasoo" \
            --trigger scheduled \
            --limit "$LIMIT" \
            --concurrency "$CONCURRENCY" \
            $VERBOSE_FLAG
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SECRET_KEY: ${{ secrets.SUPABASE_SECRET_KEY }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}

      - name: Upload execution logs (on failure)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: crawler-logs-sulwhasoo-${{ github.run_number }}
          path: |
            crawler/cj/*.log
            crawler/cj/debug_*.html
            crawler/cj/output/*.json
          retention-days: 7
          if-no-files-found: ignore
