#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¹´ì¹´ì˜¤ ë¼ì´ë¸Œ ì‡¼í•‘ í¬ë¡¤ëŸ¬
ë·°í‹° ì¹´í…Œê³ ë¦¬ì—ì„œ íŠ¹ì • ë¸Œëœë“œ ë°©ì†¡ ìˆ˜ì§‘
"""

import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import time
import json
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ìˆ˜ì§‘ ëŒ€ìƒ ë¸Œëœë“œ
TARGET_BRANDS = [
    'ì„¤í™”ìˆ˜', 'ë¼ë„¤ì¦ˆ', 'ì´ë‹ˆìŠ¤í”„ë¦¬', 'í•´í”¼ë°”ìŠ¤', 
    'ë°”ì´íƒˆë·°í‹°', 'í”„ë¦¬ë©”ë¼', 'ì˜¤ì„¤ë¡', 'ì•„ì´ì˜¤í˜', 
    'í—¤ë¼', 'ì—ìŠ¤íŠ¸ë¼'
]

class KakaoLiveCrawler:
    """ì¹´ì¹´ì˜¤ ë¼ì´ë¸Œ ì‡¼í•‘ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.base_url = 'https://shoppinglive.kakao.com'
        self.beauty_category_url = 'https://shoppinglive.kakao.com/categories?t_src=shopping_live&categoryId=4'
        self.driver = None
        self.collected_data = []
        
    def setup_driver(self):
        """Selenium WebDriver ì„¤ì •"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            logger.info("âœ… Chrome WebDriver ì´ˆê¸°í™” ì„±ê³µ")
            return True
        except Exception as e:
            logger.error(f"âŒ Chrome WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def close_driver(self):
        """WebDriver ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            logger.info("âœ… Chrome WebDriver ì¢…ë£Œ")
    
    def scroll_to_load_all(self):
        """í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ"""
        try:
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            
            while True:
                # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)  # ë¡œë”© ëŒ€ê¸°
                
                # ìƒˆë¡œìš´ ë†’ì´ ê³„ì‚°
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                
                if new_height == last_height:
                    break
                    
                last_height = new_height
                
            logger.info("âœ… í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë¡¤ ì˜¤ë¥˜: {e}")
            return False
    
    def extract_live_items(self):
        """ë¼ì´ë¸Œ ë°©ì†¡ ì•„ì´í…œ ì¶”ì¶œ"""
        try:
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # ë¼ì´ë¸Œ ë°©ì†¡ ì¹´ë“œ ì°¾ê¸° (ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
            live_items = []
            
            # ì¹´ì¹´ì˜¤ ë¼ì´ë¸Œ ì‡¼í•‘ì˜ ì‹¤ì œ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ì„ íƒì ì‚¬ìš©
            # ì˜ˆì‹œ: ë¼ì´ë¸Œ ì¹´ë“œëŠ” íŠ¹ì • í´ë˜ìŠ¤ë‚˜ ë°ì´í„° ì†ì„±ì„ ê°€ì§ˆ ìˆ˜ ìˆìŒ
            cards = soup.find_all('a', href=True)
            
            for card in cards:
                href = card.get('href', '')
                if '/lives/' in href or '/live/' in href:
                    # ë¼ì´ë¸Œ ë°©ì†¡ ë§í¬ ë°œê²¬
                    live_url = self.base_url + href if not href.startswith('http') else href
                    
                    # ì œëª© ì¶”ì¶œ
                    title = ''
                    title_elem = card.find(['h3', 'h4', 'span', 'div'], class_=lambda x: x and ('title' in x.lower() or 'name' in x.lower()))
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                    
                    # ë¸Œëœë“œ í™•ì¸
                    is_target_brand = any(brand in title for brand in TARGET_BRANDS)
                    
                    if is_target_brand:
                        item = {
                            'url': live_url,
                            'title': title,
                            'found_brand': next((brand for brand in TARGET_BRANDS if brand in title), ''),
                            'platform': 'ì¹´ì¹´ì˜¤'
                        }
                        live_items.append(item)
                        logger.info(f"ğŸ“¦ ë°œê²¬: {title}")
            
            logger.info(f"âœ… ì´ {len(live_items)}ê°œ ë¼ì´ë¸Œ ë°©ì†¡ ë°œê²¬")
            return live_items
            
        except Exception as e:
            logger.error(f"âŒ ë¼ì´ë¸Œ ì•„ì´í…œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def collect_live_detail(self, live_url, title, brand):
        """ê°œë³„ ë¼ì´ë¸Œ ë°©ì†¡ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
        try:
            logger.info(f"ğŸ” ìƒì„¸ ì •ë³´ ìˆ˜ì§‘: {title}")
            
            self.driver.get(live_url)
            time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ (ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
            detail_data = {
                'metadata': {
                    'live_id': f"KAKAO_{brand.upper()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    'platform_name': 'ì¹´ì¹´ì˜¤',
                    'brand_name': brand,
                    'live_title_customer': title,
                    'live_title_cs': f'{brand} ì¹´ì¹´ì˜¤ ë¼ì´ë¸Œ',
                    'source_url': live_url,
                    'thumbnail_url': '',
                    'status': self._determine_status(soup),
                    'collected_at': datetime.now().isoformat(),
                    'is_real_data': True
                },
                'schedule': self._extract_schedule(soup),
                'products': self._extract_products(soup),
                'benefits': self._extract_benefits(soup),
                'duplicate_policy': {},
                'restrictions': {},
                'live_specific': {
                    'key_mentions': [],
                    'broadcast_qa': [],
                    'timeline': []
                },
                'cs_info': {
                    'expected_questions': [],
                    'response_scripts': [],
                    'risk_points': [],
                    'cs_note': f'{brand} ì¹´ì¹´ì˜¤ ë¼ì´ë¸Œ'
                }
            }
            
            return detail_data
            
        except Exception as e:
            logger.error(f"âŒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì˜¤ë¥˜ ({title}): {e}")
            return None
    
    def _determine_status(self, soup):
        """ë°©ì†¡ ìƒíƒœ íŒë‹¨"""
        # ì‹¤ì œ HTMLì—ì„œ ìƒíƒœ ì •ë³´ ì¶”ì¶œ
        # ì˜ˆ: LIVE, ì˜ˆì •, ë‹¤ì‹œë³´ê¸° ë“±
        return 'PENDING'  # ê¸°ë³¸ê°’
    
    def _extract_schedule(self, soup):
        """ë°©ì†¡ ì¼ì • ì¶”ì¶œ"""
        return {
            'broadcast_date': datetime.now().strftime('%Y-%m-%d'),
            'broadcast_start_time': '00:00',
            'broadcast_end_time': '00:00',
            'benefit_valid_type': 'ë°©ì†¡ ì¤‘ë§Œ',
            'benefit_start_datetime': '',
            'benefit_end_datetime': '',
            'broadcast_type': 'ë‹¨ë…ë¼ì´ë¸Œ'
        }
    
    def _extract_products(self, soup):
        """íŒë§¤ ìƒí’ˆ ì¶”ì¶œ"""
        return []
    
    def _extract_benefits(self, soup):
        """í˜œíƒ ì •ë³´ ì¶”ì¶œ"""
        return {
            'discounts': [],
            'gifts': [],
            'coupons': [],
            'shipping': []
        }
    
    def run(self):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            logger.info("="*80)
            logger.info("ğŸš€ ì¹´ì¹´ì˜¤ ë¼ì´ë¸Œ ì‡¼í•‘ í¬ë¡¤ë§ ì‹œì‘")
            logger.info("="*80)
            
            # WebDriver ì„¤ì •
            if not self.setup_driver():
                return False
            
            # ë·°í‹° ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì ‘ì†
            logger.info(f"ğŸ“± ì¹´ì¹´ì˜¤ ë·°í‹° ì¹´í…Œê³ ë¦¬ ì ‘ì†: {self.beauty_category_url}")
            self.driver.get(self.beauty_category_url)
            time.sleep(5)  # ì´ˆê¸° ë¡œë”© ëŒ€ê¸°
            
            # í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ
            self.scroll_to_load_all()
            
            # ë¼ì´ë¸Œ ë°©ì†¡ ì•„ì´í…œ ì¶”ì¶œ
            live_items = self.extract_live_items()
            
            if not live_items:
                logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë¼ì´ë¸Œ ë°©ì†¡ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ê° ë¼ì´ë¸Œ ë°©ì†¡ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            logger.info(f"ğŸ“Š {len(live_items)}ê°œ ë°©ì†¡ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘")
            
            for idx, item in enumerate(live_items, 1):
                logger.info(f"[{idx}/{len(live_items)}] ì²˜ë¦¬ ì¤‘...")
                detail = self.collect_live_detail(
                    item['url'],
                    item['title'],
                    item['found_brand']
                )
                
                if detail:
                    self.collected_data.append(detail)
                
                time.sleep(2)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            
            # ê²°ê³¼ ì €ì¥
            self.save_results()
            
            logger.info("="*80)
            logger.info(f"âœ… ì¹´ì¹´ì˜¤ ë¼ì´ë¸Œ ì‡¼í•‘ í¬ë¡¤ë§ ì™„ë£Œ: {len(self.collected_data)}ê°œ")
            logger.info("="*80)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return False
        finally:
            self.close_driver()
    
    def save_results(self):
        """ìˆ˜ì§‘ ê²°ê³¼ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'kakao_live_beauty_{timestamp}.json'
            filepath = f'/Users/amore/ai_cs ì‹œìŠ¤í…œ/crawler/data/{filename}'
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(self.collected_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {filepath}")
            
            # ìš”ì•½ ì •ë³´ ì¶œë ¥
            self._print_summary()
            
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def _print_summary(self):
        """ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½"""
        print("\n" + "="*80)
        print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        # ë¸Œëœë“œë³„ ì§‘ê³„
        brand_counts = {}
        for data in self.collected_data:
            brand = data['metadata']['brand_name']
            brand_counts[brand] = brand_counts.get(brand, 0) + 1
        
        for brand, count in sorted(brand_counts.items()):
            print(f"   {brand}: {count}ê°œ")
        
        print(f"\n   ì´ {len(self.collected_data)}ê°œ ë°©ì†¡ ìˆ˜ì§‘")
        print("="*80)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    crawler = KakaoLiveCrawler()
    success = crawler.run()
    
    if success:
        print("\nâœ… ì¹´ì¹´ì˜¤ ë¼ì´ë¸Œ ì‡¼í•‘ í¬ë¡¤ë§ ì„±ê³µ!")
    else:
        print("\nâŒ ì¹´ì¹´ì˜¤ ë¼ì´ë¸Œ ì‡¼í•‘ í¬ë¡¤ë§ ì‹¤íŒ¨!")
    
    return 0 if success else 1


if __name__ == '__main__':
    exit(main())

