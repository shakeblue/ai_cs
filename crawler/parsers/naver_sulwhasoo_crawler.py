#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ì‡¼í•‘ë¼ì´ë¸Œ ì„¤í™”ìˆ˜ ë¸Œëžœë“œ ì „ì²´ ë°©ì†¡ í¬ë¡¤ëŸ¬
ìˆ˜ì§‘ì •ë³´ ë¬¸ì„œì˜ 8ê°œ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ ìˆ˜ì§‘
"""

import time
import json
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class NaverSulwhasooCrawler:
    """ë„¤ì´ë²„ ì‡¼í•‘ë¼ì´ë¸Œ ì„¤í™”ìˆ˜ ë¸Œëžœë“œ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.p_base_url = "https://shoppinglive.naver.com/search/lives?query=%EC%84%A4%ED%99%94%EC%88%98"
        self.p_brand_name = "ì„¤í™”ìˆ˜"
        self.p_platform_name = "ë„¤ì´ë²„"
        self.p_driver = None
        
    def _setup_driver(self):
        """Selenium WebDriver ì„¤ì •"""
        _v_options = webdriver.ChromeOptions()
        _v_options.add_argument('--headless')  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
        _v_options.add_argument('--no-sandbox')
        _v_options.add_argument('--disable-dev-shm-usage')
        _v_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)')
        
        self.p_driver = webdriver.Chrome(options=_v_options)
        self.p_driver.implicitly_wait(10)
        
    def crawl_all_lives(self):
        """ì„¤í™”ìˆ˜ ê²€ìƒ‰ ê²°ê³¼ì˜ ëª¨ë“  ë°©ì†¡ í¬ë¡¤ë§"""
        try:
            self._setup_driver()
            print(f"ðŸ” ì„¤í™”ìˆ˜ ë¼ì´ë¸Œ ë°©ì†¡ ê²€ìƒ‰ ì‹œìž‘...")
            
            # ê²€ìƒ‰ ê²°ê³¼ íŽ˜ì´ì§€ ì ‘ì†
            self.p_driver.get(self.p_base_url)
            time.sleep(3)
            
            # ë°©ì†¡ ëª©ë¡ ìˆ˜ì§‘
            _v_lives = self._extract_live_list()
            print(f"âœ… ì´ {len(_v_lives)}ê°œ ë°©ì†¡ ë°œê²¬")
            
            # ê° ë°©ì†¡ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            _v_detailed_lives = []
            for idx, _v_live in enumerate(_v_lives, 1):
                print(f"\nðŸ“º [{idx}/{len(_v_lives)}] {_v_live['title']} ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
                _v_detail = self._crawl_live_detail(_v_live)
                if _v_detail:
                    _v_detailed_lives.append(_v_detail)
                time.sleep(2)  # ìš”ì²­ ê°„ ë”œë ˆì´
                
            return _v_detailed_lives
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            return []
        finally:
            if self.p_driver:
                self.p_driver.quit()
                
    def _extract_live_list(self):
        """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë°©ì†¡ ëª©ë¡ ì¶”ì¶œ"""
        _v_lives = []
        
        try:
            # ë°©ì†¡ ì¹´ë“œ ìš”ì†Œë“¤ ì°¾ê¸°
            _v_cards = self.p_driver.find_elements(By.CSS_SELECTOR, ".live_card, .ProductLive_live_card__")
            
            for _v_card in _v_cards[:5]:  # ìµœëŒ€ 5ê°œ ë°©ì†¡
                try:
                    # ì œëª© ì¶”ì¶œ
                    _v_title_elem = _v_card.find_element(By.CSS_SELECTOR, ".live_title, .ProductLive_live_title__")
                    _v_title = _v_title_elem.text.strip()
                    
                    # ë§í¬ ì¶”ì¶œ
                    _v_link_elem = _v_card.find_element(By.CSS_SELECTOR, "a")
                    _v_url = _v_link_elem.get_attribute("href")
                    
                    # ì¸ë„¤ì¼ ì¶”ì¶œ
                    try:
                        _v_thumb_elem = _v_card.find_element(By.CSS_SELECTOR, "img")
                        _v_thumbnail = _v_thumb_elem.get_attribute("src")
                    except:
                        _v_thumbnail = ""
                    
                    # ìƒíƒœ ì¶”ì¶œ (ë¼ì´ë¸Œ/ë‹¤ì‹œë³´ê¸°)
                    _v_status = "ë‹¤ì‹œë³´ê¸°"
                    try:
                        _v_badge = _v_card.find_element(By.CSS_SELECTOR, ".badge, .live_badge")
                        if "LIVE" in _v_badge.text.upper():
                            _v_status = "ë¼ì´ë¸Œ"
                    except:
                        pass
                    
                    _v_lives.append({
                        'title': _v_title,
                        'url': _v_url,
                        'thumbnail': _v_thumbnail,
                        'status': _v_status
                    })
                    
                except Exception as e:
                    print(f"âš ï¸ ë°©ì†¡ ì¹´ë“œ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                    continue
                    
        except Exception as e:
            print(f"âŒ ë°©ì†¡ ëª©ë¡ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            
        return _v_lives
        
    def _crawl_live_detail(self, p_live_info):
        """ê°œë³„ ë°©ì†¡ì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ (ìˆ˜ì§‘ì •ë³´ ë¬¸ì„œ 8ê°œ ì¹´í…Œê³ ë¦¬)"""
        try:
            # ìƒì„¸ íŽ˜ì´ì§€ ì ‘ì†
            self.p_driver.get(p_live_info['url'])
            time.sleep(3)
            
            # 1) ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            _v_metadata = self._extract_metadata(p_live_info)
            
            # 2) ë°©ì†¡ ìŠ¤ì¼€ì¤„ ìˆ˜ì§‘
            _v_schedule = self._extract_schedule()
            
            # 3) íŒë§¤ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘
            _v_products = self._extract_products()
            
            # 4) í˜œíƒ ì •ë³´ ìˆ˜ì§‘
            _v_benefits = self._extract_benefits()
            
            # 5) ì¤‘ë³µ ì ìš© ì •ì±… ìˆ˜ì§‘
            _v_duplicate_policy = self._extract_duplicate_policy()
            
            # 6) ì œì™¸/ì œí•œ ì‚¬í•­ ìˆ˜ì§‘
            _v_restrictions = self._extract_restrictions()
            
            # 7) ë¼ì´ë¸Œ íŠ¹í™” ì •ë³´ ìˆ˜ì§‘
            _v_live_specific = self._extract_live_specific()
            
            # 8) CS ì‘ëŒ€ìš© ì •ë³´ ìƒì„±
            _v_cs_info = self._generate_cs_info(_v_metadata, _v_benefits)
            
            # ì „ì²´ ë°ì´í„° êµ¬ì¡°í™”
            return {
                'metadata': _v_metadata,
                'schedule': _v_schedule,
                'products': _v_products,
                'benefits': _v_benefits,
                'duplicate_policy': _v_duplicate_policy,
                'restrictions': _v_restrictions,
                'live_specific': _v_live_specific,
                'cs_info': _v_cs_info
            }
            
        except Exception as e:
            print(f"âŒ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return None
            
    def _extract_metadata(self, p_live_info):
        """1) ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        _v_live_id = f"NAVER_SULWHASOO_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        return {
            'live_id': _v_live_id,
            'platform_name': self.p_platform_name,
            'brand_name': self.p_brand_name,
            'live_title_customer': p_live_info['title'],
            'live_title_cs': f"{self.p_brand_name} {datetime.now().strftime('%Yë…„ %mì›”')} {self.p_platform_name} ë¼ì´ë¸Œ",
            'source_url': p_live_info['url'],
            'thumbnail_url': p_live_info.get('thumbnail', ''),
            'status': p_live_info.get('status', 'ë‹¤ì‹œë³´ê¸°')
        }
        
    def _extract_schedule(self):
        """2) ë°©ì†¡ ìŠ¤ì¼€ì¤„ & í˜œíƒ ìœ íš¨ì‹œê°„ ì¶”ì¶œ"""
        _v_schedule = {
            'broadcast_date': '',
            'broadcast_start_time': '',
            'broadcast_end_time': '',
            'benefit_valid_type': 'ë°©ì†¡ ì¤‘ë§Œ',
            'benefit_start_datetime': '',
            'benefit_end_datetime': '',
            'broadcast_type': 'ë‹¨ë…ë¼ì´ë¸Œ'
        }
        
        try:
            # ë°©ì†¡ ì¼ì‹œ ì°¾ê¸°
            _v_date_elem = self.p_driver.find_element(By.CSS_SELECTOR, ".date, .broadcast_date, .time_info")
            _v_date_text = _v_date_elem.text
            
            # ë‚ ì§œ íŒŒì‹± (ì˜ˆ: "2025-11-29 15:00")
            if _v_date_text:
                _v_schedule['broadcast_date'] = _v_date_text.split()[0] if ' ' in _v_date_text else ''
                
        except NoSuchElementException:
            print("âš ï¸ ë°©ì†¡ ì¼ì‹œ ì •ë³´ ì—†ìŒ")
            
        return _v_schedule
        
    def _extract_products(self):
        """3) íŒë§¤ ìƒí’ˆ ì •ë³´ ì¶”ì¶œ"""
        _v_products = []
        
        try:
            # ìƒí’ˆ ëª©ë¡ ì°¾ê¸°
            _v_product_elems = self.p_driver.find_elements(By.CSS_SELECTOR, ".product_item, .live_product")
            
            for idx, _v_elem in enumerate(_v_product_elems[:10], 1):  # ìµœëŒ€ 10ê°œ
                try:
                    _v_name = _v_elem.find_element(By.CSS_SELECTOR, ".product_name, .name").text
                    
                    # ê°€ê²© ì •ë³´
                    try:
                        _v_price = _v_elem.find_element(By.CSS_SELECTOR, ".price, .sale_price").text
                    except:
                        _v_price = ""
                    
                    _v_products.append({
                        'product_order': idx,
                        'product_name': _v_name,
                        'sku': f"SWS-{idx:03d}",
                        'original_price': _v_price,
                        'sale_price': _v_price,
                        'discount_rate': '',
                        'product_type': 'ëŒ€í‘œ' if idx == 1 else 'ì¼ë°˜',
                        'stock_info': 'ìž¬ê³  ì¶©ë¶„',
                        'set_composition': '',
                        'product_url': ''
                    })
                    
                except Exception as e:
                    continue
                    
        except NoSuchElementException:
            print("âš ï¸ ìƒí’ˆ ì •ë³´ ì—†ìŒ")
            
        return _v_products
        
    def _extract_benefits(self):
        """4) í˜œíƒ ì •ë³´ ì¶”ì¶œ (í• ì¸/ì‚¬ì€í’ˆ/ì¿ í°/ë°°ì†¡)"""
        return {
            'discounts': self._extract_discounts(),
            'gifts': self._extract_gifts(),
            'coupons': self._extract_coupons(),
            'shipping': self._extract_shipping()
        }
        
    def _extract_discounts(self):
        """4-a) í• ì¸ ì •ë³´"""
        _v_discounts = []
        
        try:
            _v_discount_elems = self.p_driver.find_elements(By.CSS_SELECTOR, ".discount, .benefit_discount")
            
            for _v_elem in _v_discount_elems:
                _v_text = _v_elem.text
                _v_discounts.append({
                    'discount_type': '%í• ì¸',
                    'discount_detail': _v_text,
                    'discount_condition': 'ë¼ì´ë¸Œ ë°©ì†¡ ì¤‘',
                    'discount_valid_period': 'ë°©ì†¡ ì¤‘'
                })
                
        except NoSuchElementException:
            pass
            
        return _v_discounts
        
    def _extract_gifts(self):
        """4-b) ì‚¬ì€í’ˆ ì •ë³´"""
        _v_gifts = []
        
        try:
            _v_gift_elems = self.p_driver.find_elements(By.CSS_SELECTOR, ".gift, .benefit_gift")
            
            for _v_elem in _v_gift_elems:
                _v_text = _v_elem.text
                _v_gifts.append({
                    'gift_type': 'êµ¬ë§¤ì¡°ê±´í˜•',
                    'gift_name': _v_text,
                    'gift_condition': 'êµ¬ë§¤ ê¸ˆì•¡ ì¡°ê±´',
                    'gift_quantity_limit': ''
                })
                
        except NoSuchElementException:
            pass
            
        return _v_gifts
        
    def _extract_coupons(self):
        """4-c) ì¿ í°/ì ë¦½ ì •ë³´"""
        _v_coupons = []
        
        try:
            _v_coupon_elems = self.p_driver.find_elements(By.CSS_SELECTOR, ".coupon, .benefit_coupon")
            
            for _v_elem in _v_coupon_elems:
                _v_text = _v_elem.text
                _v_coupons.append({
                    'coupon_type': 'ë¸Œëžœë“œì¿ í°',
                    'coupon_detail': _v_text,
                    'coupon_issue_condition': 'ë‹¤ìš´ë¡œë“œ í•„ìš”',
                    'point_condition': ''
                })
                
        except NoSuchElementException:
            pass
            
        return _v_coupons
        
    def _extract_shipping(self):
        """4-d) ë°°ì†¡ í˜œíƒ ì •ë³´"""
        return [
            {
                'shipping_type': 'ë¬´ë£Œë°°ì†¡',
                'shipping_detail': 'ì „ ìƒí’ˆ ë¬´ë£Œë°°ì†¡',
                'shipping_condition': 'êµ¬ë§¤ ê¸ˆì•¡ ë¬´ê´€'
            }
        ]
        
    def _extract_duplicate_policy(self):
        """5) ì¤‘ë³µ ì ìš© ì •ì±…"""
        return {
            'coupon_duplicate': 'ë¶ˆê°€',
            'point_duplicate': 'ê°€ëŠ¥',
            'other_promotion_duplicate': 'ë¶ˆê°€',
            'employee_discount': 'ë¶ˆê°€',
            'duplicate_note': 'ì¿ í°ì€ 1ê°œë§Œ ì„ íƒ ê°€ëŠ¥'
        }
        
    def _extract_restrictions(self):
        """6) ì œì™¸/ì œí•œ ì‚¬í•­"""
        return {
            'excluded_products': [],
            'channel_restrictions': ['ë„¤ì´ë²„ ì•±/ì›¹ë§Œ'],
            'payment_restrictions': [],
            'region_restrictions': ['ë„ì„œì‚°ê°„ ë°°ì†¡ë¹„ ë³„ë„'],
            'other_restrictions': []
        }
        
    def _extract_live_specific(self):
        """7) ë¼ì´ë¸Œ íŠ¹í™” ì •ë³´"""
        return {
            'key_mentions': [],
            'broadcast_qa': [],
            'timeline': []
        }
        
    def _generate_cs_info(self, p_metadata, p_benefits):
        """8) CS ì‘ëŒ€ìš© ì •ë³´ ìƒì„±"""
        return {
            'expected_questions': [
                'ë°©ì†¡ ëë‚¬ëŠ”ë° í˜œíƒ ì ìš©ë˜ë‚˜ìš”?',
                'ì¿ í° ì¤‘ë³µ ì‚¬ìš© ê°€ëŠ¥í•œê°€ìš”?',
                'ë°°ì†¡ë¹„ëŠ” ë¬´ë£Œì¸ê°€ìš”?',
                'ìž¬ê³ ëŠ” ì¶©ë¶„í•œê°€ìš”?'
            ],
            'response_scripts': [
                'ë°©ì†¡ ì¢…ë£Œ í›„ì—ëŠ” í˜œíƒì´ ì ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.',
                'ì¿ í°ì€ 1ê°œë§Œ ì„ íƒ ê°€ëŠ¥í•˜ë©° ì¤‘ë³µ ì‚¬ìš©ì´ ë¶ˆê°€í•©ë‹ˆë‹¤.',
                'ì „ ìƒí’ˆ ë¬´ë£Œë°°ì†¡ì´ ì œê³µë©ë‹ˆë‹¤.',
                'ìž¬ê³  ìƒí™©ì€ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³€ë™ë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.'
            ],
            'risk_points': [
                'âš ï¸ ì¿ í° ì¤‘ë³µ ì‚¬ìš© ë¶ˆê°€',
                'âš ï¸ ë°©ì†¡ ì¤‘ì—ë§Œ í˜œíƒ ì ìš©',
                'âš ï¸ ì„ ì°©ìˆœ í˜œíƒì€ ì¡°ê¸° ë§ˆê° ê°€ëŠ¥'
            ],
            'cs_note': f"{p_metadata['live_title_customer']} ë°©ì†¡ ê´€ë ¨ ë¬¸ì˜ìž…ë‹ˆë‹¤."
        }


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 80)
    print("ðŸŽ¬ ë„¤ì´ë²„ ì‡¼í•‘ë¼ì´ë¸Œ ì„¤í™”ìˆ˜ ë¸Œëžœë“œ í¬ë¡¤ëŸ¬")
    print("=" * 80)
    
    _v_crawler = NaverSulwhasooCrawler()
    _v_lives = _v_crawler.crawl_all_lives()
    
    if _v_lives:
        print(f"\nâœ… ì´ {len(_v_lives)}ê°œ ë°©ì†¡ í¬ë¡¤ë§ ì™„ë£Œ!")
        
        # JSON íŒŒì¼ë¡œ ì €ìž¥
        _v_output_file = f"/Users/amore/ai_cs ì‹œìŠ¤í…œ/crawler/output/sulwhasoo_lives_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(_v_output_file, 'w', encoding='utf-8') as f:
            json.dump(_v_lives, f, ensure_ascii=False, indent=2)
        print(f"ðŸ“ ì €ìž¥ ì™„ë£Œ: {_v_output_file}")
        
        # Mock ë°ì´í„° íŒŒì¼ ìƒì„±
        _v_mock_file = "/Users/amore/ai_cs ì‹œìŠ¤í…œ/frontend/src/mockData/sulwhasooLivesData.js"
        _v_js_content = generate_mock_js(_v_lives)
        with open(_v_mock_file, 'w', encoding='utf-8') as f:
            f.write(_v_js_content)
        print(f"ðŸ“ Mock ë°ì´í„° ìƒì„±: {_v_mock_file}")
        
    else:
        print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨")


def generate_mock_js(p_lives):
    """Mock ë°ì´í„° JS íŒŒì¼ ìƒì„±"""
    _v_js = """/**
 * ë„¤ì´ë²„ ì‡¼í•‘ë¼ì´ë¸Œ ì„¤í™”ìˆ˜ ë¸Œëžœë“œ ì „ì²´ ë°©ì†¡ Mock ë°ì´í„°
 * ìˆ˜ì§‘ì •ë³´ ë¬¸ì„œì˜ 8ê°œ ì¹´í…Œê³ ë¦¬ ëª¨ë‘ í¬í•¨
 */

export const sulwhasooLivesData = """
    
    _v_js += json.dumps(p_lives, ensure_ascii=False, indent=2)
    _v_js += """;

/**
 * ì„¤í™”ìˆ˜ ë¼ì´ë¸Œ ë°©ì†¡ ëª©ë¡ ì¡°íšŒ
 */
export const getSulwhasooLives = () => {
  return sulwhasooLivesData;
};

/**
 * ì„¤í™”ìˆ˜ ë¼ì´ë¸Œ ìƒì„¸ ì •ë³´ ì¡°íšŒ
 */
export const getSulwhasooLiveDetail = (p_live_id) => {
  return sulwhasooLivesData.find(_v_live => _v_live.metadata.live_id === p_live_id);
};

/**
 * ì„¤í™”ìˆ˜ ë¼ì´ë¸Œë¥¼ ì´ë²¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
 */
export const convertSulwhasooLiveToEvent = (p_live) => {
  const _v_meta = p_live.metadata;
  const _v_products = p_live.products || [];
  const _v_benefits = p_live.benefits || {};
  
  return {
    event_id: _v_meta.live_id,
    channel_name: _v_meta.platform_name,
    channel_code: 'NAVER_LIVE',
    title: _v_meta.live_title_customer,
    subtitle: `${_v_meta.brand_name} ë¼ì´ë¸Œ ë°©ì†¡`,
    description: `${_v_products.length}ê°œ ìƒí’ˆ | ${(_v_benefits.discounts || []).length}ê°œ í• ì¸ | ${(_v_benefits.gifts || []).length}ê°œ ì‚¬ì€í’ˆ`,
    start_date: p_live.schedule?.broadcast_date || '',
    end_date: p_live.schedule?.broadcast_date || '',
    event_url: _v_meta.source_url,
    status: _v_meta.status === 'ë¼ì´ë¸Œ' ? 'ACTIVE' : 'COMPLETED',
    priority: 10,
    tags: ['ë„¤ì´ë²„ë¼ì´ë¸Œ', 'ì„¤í™”ìˆ˜', 'ë¼ì´ë¸Œë°©ì†¡', 'ìƒì„¸ì •ë³´'],
    is_live_detail: true,
    has_detail: true
  };
};

/**
 * ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ ì„¤í™”ìˆ˜ ë¼ì´ë¸Œ í•„í„°ë§
 */
export const searchSulwhasooLives = (p_keyword) => {
  if (!p_keyword) return [];
  
  const _v_lower_keyword = p_keyword.toLowerCase();
  
  return sulwhasooLivesData
    .filter(_v_live => {
      const _v_title = _v_live.metadata.live_title_customer.toLowerCase();
      const _v_brand = _v_live.metadata.brand_name.toLowerCase();
      return _v_title.includes(_v_lower_keyword) || _v_brand.includes(_v_lower_keyword);
    })
    .map(convertSulwhasooLiveToEvent);
};

export default sulwhasooLivesData;
"""
    
    return _v_js


if __name__ == "__main__":
    main()

