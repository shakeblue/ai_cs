#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ì‡¼í•‘ë¼ì´ë¸Œ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬
1ì‹œê°„ë§ˆë‹¤ ìë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import schedule
import time
import logging
import json
import subprocess
from datetime import datetime
from pathlib import Path

# ë¡œê¹… ì„¤ì •
log_dir = Path(__file__).parent / 'logs'
log_dir.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(log_dir / f'scheduler_{datetime.now().strftime("%Y%m%d")}.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


class DataCollectionScheduler:
    """ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”"""
        self.p_crawler_dir = Path(__file__).parent
        self.p_output_dir = self.p_crawler_dir / 'output'
        self.p_output_dir.mkdir(exist_ok=True)
        
        # ìˆ˜ì§‘ í†µê³„
        self.p_stats = {
            'total_runs': 0,
            'successful_runs': 0,
            'failed_runs': 0,
            'last_run': None,
            'last_success': None,
            'last_error': None
        }
        
    def collect_sulwhasoo_data(self):
        """ì„¤í™”ìˆ˜ ë¸Œëœë“œ ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("=" * 80)
        logger.info("ğŸ”„ ì„¤í™”ìˆ˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        logger.info("=" * 80)
        
        try:
            self.p_stats['total_runs'] += 1
            self.p_stats['last_run'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # ì„¤í™”ìˆ˜ í¬ë¡¤ëŸ¬ ì‹¤í–‰
            _v_script = self.p_crawler_dir / 'parsers' / 'naver_sulwhasoo_crawler.py'
            
            if not _v_script.exists():
                logger.warning(f"âš ï¸ í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤: {_v_script}")
                logger.info("ğŸ’¡ ëŒ€ì‹  ë©€í‹° ë¸Œëœë“œ í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
                _v_script = self.p_crawler_dir / 'crawl_multi_brands.py'
            
            if _v_script.exists():
                logger.info(f"ğŸ“ ì‹¤í–‰: {_v_script}")
                _v_result = subprocess.run(
                    ['python3', str(_v_script)],
                    cwd=str(self.p_crawler_dir),
                    capture_output=True,
                    text=True,
                    timeout=600  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                )
                
                if _v_result.returncode == 0:
                    logger.info("âœ… ì„¤í™”ìˆ˜ ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ!")
                    logger.info(f"ì¶œë ¥:\n{_v_result.stdout}")
                    self.p_stats['successful_runs'] += 1
                    self.p_stats['last_success'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                else:
                    logger.error(f"âŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨ (ì½”ë“œ: {_v_result.returncode})")
                    logger.error(f"ì—ëŸ¬:\n{_v_result.stderr}")
                    self.p_stats['failed_runs'] += 1
                    self.p_stats['last_error'] = _v_result.stderr[:500]
            else:
                logger.error(f"âŒ í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {_v_script}")
                self.p_stats['failed_runs'] += 1
                
        except subprocess.TimeoutExpired:
            logger.error("âŒ í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ (10ë¶„ ì´ˆê³¼)")
            self.p_stats['failed_runs'] += 1
        except Exception as e:
            logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            self.p_stats['failed_runs'] += 1
            self.p_stats['last_error'] = str(e)
        finally:
            self._save_stats()
            logger.info("=" * 80)
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ | ì„±ê³µ: {self.p_stats['successful_runs']}, ì‹¤íŒ¨: {self.p_stats['failed_runs']}")
            logger.info("=" * 80)
            
    def collect_all_brands_data(self):
        """ëª¨ë“  ë¸Œëœë“œ ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("=" * 80)
        logger.info("ğŸ”„ ì „ì²´ ë¸Œëœë“œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        logger.info("=" * 80)
        
        try:
            # ë©€í‹° ë¸Œëœë“œ í¬ë¡¤ëŸ¬ ì‹¤í–‰
            _v_script = self.p_crawler_dir / 'crawl_multi_brands.py'
            
            if _v_script.exists():
                logger.info(f"ğŸ“ ì‹¤í–‰: {_v_script}")
                _v_result = subprocess.run(
                    ['python3', str(_v_script)],
                    cwd=str(self.p_crawler_dir),
                    capture_output=True,
                    text=True,
                    timeout=1800  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
                )
                
                if _v_result.returncode == 0:
                    logger.info("âœ… ì „ì²´ ë¸Œëœë“œ ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ!")
                    logger.info(f"ì¶œë ¥:\n{_v_result.stdout}")
                else:
                    logger.error(f"âŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨ (ì½”ë“œ: {_v_result.returncode})")
                    logger.error(f"ì—ëŸ¬:\n{_v_result.stderr}")
            else:
                logger.error(f"âŒ ë©€í‹° ë¸Œëœë“œ í¬ë¡¤ëŸ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {_v_script}")
                
        except Exception as e:
            logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        finally:
            logger.info("=" * 80)
            
    def _save_stats(self):
        """ìˆ˜ì§‘ í†µê³„ ì €ì¥"""
        try:
            _v_stats_file = self.p_output_dir / 'scheduler_stats.json'
            with open(_v_stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.p_stats, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"í†µê³„ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            
    def get_stats(self):
        """ìˆ˜ì§‘ í†µê³„ ì¶œë ¥"""
        logger.info("=" * 80)
        logger.info("ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ í†µê³„")
        logger.info("=" * 80)
        logger.info(f"ì´ ì‹¤í–‰ íšŸìˆ˜: {self.p_stats['total_runs']}")
        logger.info(f"ì„±ê³µ: {self.p_stats['successful_runs']}")
        logger.info(f"ì‹¤íŒ¨: {self.p_stats['failed_runs']}")
        logger.info(f"ë§ˆì§€ë§‰ ì‹¤í–‰: {self.p_stats['last_run']}")
        logger.info(f"ë§ˆì§€ë§‰ ì„±ê³µ: {self.p_stats['last_success']}")
        if self.p_stats['last_error']:
            logger.info(f"ë§ˆì§€ë§‰ ì—ëŸ¬: {self.p_stats['last_error'][:200]}")
        logger.info("=" * 80)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("=" * 80)
    logger.info("ğŸš€ ë„¤ì´ë²„ ì‡¼í•‘ë¼ì´ë¸Œ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
    logger.info("=" * 80)
    logger.info("â° ìˆ˜ì§‘ ì£¼ê¸°: 1ì‹œê°„ë§ˆë‹¤")
    logger.info("ğŸ“¦ ìˆ˜ì§‘ ëŒ€ìƒ: ì„¤í™”ìˆ˜ ë¸Œëœë“œ")
    logger.info("=" * 80)
    
    scheduler = DataCollectionScheduler()
    
    # ì‹œì‘ ì‹œ ì¦‰ì‹œ 1íšŒ ì‹¤í–‰
    logger.info("ğŸ”„ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰...")
    scheduler.collect_sulwhasoo_data()
    
    # ë§¤ ì‹œê°„ ì •ê°ì— ì‹¤í–‰ (ì˜ˆ: 10:00, 11:00, 12:00...)
    schedule.every().hour.at(":00").do(scheduler.collect_sulwhasoo_data)
    
    # ë˜ëŠ” 1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰ (ì‹œì‘ ì‹œì ë¶€í„° 1ì‹œê°„ ê°„ê²©)
    # schedule.every(1).hours.do(scheduler.collect_sulwhasoo_data)
    
    # ë§¤ì¼ ìì •ì— í†µê³„ ì¶œë ¥
    schedule.every().day.at("00:00").do(scheduler.get_stats)
    
    logger.info("âœ… ìŠ¤ì¼€ì¤„ëŸ¬ ì¤€ë¹„ ì™„ë£Œ")
    logger.info("ğŸ’¡ Ctrl+Cë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    logger.info("=" * 80)
    
    try:
        while True:
            schedule.run_pending()
            time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ ì²´í¬
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸  ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ")
        scheduler.get_stats()


if __name__ == "__main__":
    main()

