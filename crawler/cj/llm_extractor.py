#!/usr/bin/env python3
"""
LLM-Based Event Information Extractor
Supports multiple providers: Claude Haiku, GPT-4o Mini, Gemini Flash
"""

import os
import json
import re
from typing import Dict, Optional, List
from enum import Enum

# Import LLM clients (all optional)
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    print("âš  Anthropic not installed. Install with: pip install anthropic")

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš  OpenAI not installed. Install with: pip install openai")

try:
    import google.generativeai as genai
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False
    print("âš  Google Generative AI not installed. Install with: pip install google-generativeai")


class LLMProvider(Enum):
    """Available LLM providers"""
    CLAUDE_HAIKU = "claude-haiku"
    GPT_4O_MINI = "gpt-5-mini"
    GEMINI_FLASH = "gemini-flash"


class LLMExtractor:
    """Extract event information using LLM APIs"""

    # Model identifiers
    MODELS = {
        LLMProvider.CLAUDE_HAIKU: "claude-3-5-haiku-20241022",
        LLMProvider.GPT_4O_MINI: "gpt-4o-mini",
        LLMProvider.GEMINI_FLASH: "gemini-1.5-flash",
    }

    # Cost per million tokens (input, output)
    COSTS = {
        LLMProvider.CLAUDE_HAIKU: (0.80, 4.00),
        LLMProvider.GPT_4O_MINI: (0.15, 0.60),
        LLMProvider.GEMINI_FLASH: (0.075, 0.30),
    }

    def __init__(self, provider: LLMProvider = LLMProvider.GPT_4O_MINI):
        """
        Initialize LLM extractor

        Args:
            provider: Which LLM provider to use (default: GPT-4o Mini)
        """
        self.provider = provider
        self.model = self.MODELS[provider]

        # Initialize appropriate client
        if provider == LLMProvider.CLAUDE_HAIKU:
            if not ANTHROPIC_AVAILABLE:
                raise ImportError("Anthropic library not installed")
            api_key = os.environ.get("ANTHROPIC_API_KEY")
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEY environment variable not set")
            self.client = anthropic.Anthropic(api_key=api_key)

        elif provider == LLMProvider.GPT_4O_MINI:
            if not OPENAI_AVAILABLE:
                raise ImportError("OpenAI library not installed")
            api_key = os.environ.get("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY environment variable not set")
            self.client = openai.OpenAI(api_key=api_key)

        elif provider == LLMProvider.GEMINI_FLASH:
            if not GOOGLE_AVAILABLE:
                raise ImportError("Google Generative AI library not installed")
            api_key = os.environ.get("GOOGLE_API_KEY")
            if not api_key:
                raise ValueError("GOOGLE_API_KEY environment variable not set")
            genai.configure(api_key=api_key)
            self.client = genai.GenerativeModel(self.model)

        print(f"âœ“ LLM Extractor initialized: {provider.value} ({self.model})")

    def _build_prompt(self, ocr_text: str) -> str:
        """Build extraction prompt for Korean event data"""
        return f"""ë‹¤ìŒì€ Naver Brand ì´ë²¤íŠ¸ í˜ì´ì§€ì˜ OCR í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ì´ í…ìŠ¤íŠ¸ì—ì„œ ì´ë²¤íŠ¸ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

OCR í…ìŠ¤íŠ¸:
{ocr_text}

ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”:

1. event_title: ì´ë²¤íŠ¸ ì œëª© (ê°€ì¥ ëˆˆì— ë„ëŠ” ì œëª© ë˜ëŠ” ê¸°íšì „ ì´ë¦„)

2. event_date: ì´ë²¤íŠ¸ ê¸°ê°„ (ë‚ ì§œ ë²”ìœ„ê°€ ìˆìœ¼ë©´ ì¶”ì¶œ)

3. benefits_by_purchase_amount: êµ¬ë§¤ ê¸ˆì•¡ë³„ í˜œíƒ ë¦¬ìŠ¤íŠ¸
   âœ… í¬í•¨í•  í•­ëª©:
   - ì‚¬ì€í’ˆ ì¦ì • (ì˜ˆ: "5ë§Œì› ì´ìƒ êµ¬ë§¤ì‹œ íŒŒìš°ì¹˜ ì¦ì •")
   - í¬ì¸íŠ¸ ì ë¦½ (ì˜ˆ: "N pay 1% ì ë¦½", "ë·°í‹°í¬ì¸íŠ¸ ì ë¦½")
   - ë¬´ë£Œ ë°°ì†¡
   - ì¶”ê°€ í• ì¸ (ë‹¨, "ì¿ í°" í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš°ë§Œ)

   âŒ ì œì™¸í•  í•­ëª©:
   - "ì¿ í°" í‚¤ì›Œë“œê°€ í¬í•¨ëœ ëª¨ë“  í•­ëª© (ì´ê±´ coupon_benefitsì— ë„£ê¸°)

4. coupon_benefits: ì¿ í° í˜œíƒ ë¦¬ìŠ¤íŠ¸
   âœ… í¬í•¨í•  í•­ëª©:
   - "ì¿ í°"ì´ë¼ëŠ” í‚¤ì›Œë“œê°€ ëª…ì‹œëœ ëª¨ë“  í•­ëª©
   - ì˜ˆ: "10% í• ì¸ ì¿ í°", "4ì²œì› ì¿ í°", "ì¥ë°”êµ¬ë‹ˆ ì¿ í°"

   âŒ ì œì™¸í•  í•­ëª©:
   - ì¿ í°ì´ ì•„ë‹Œ ì¼ë°˜ í˜œíƒ (ì‚¬ì€í’ˆ, í¬ì¸íŠ¸ ë“±)

âš ï¸ ì¤‘ìš” ê·œì¹™:
- ê° í•­ëª©ì€ í•˜ë‚˜ì˜ ì¹´í…Œê³ ë¦¬ì—ë§Œ í¬í•¨í•˜ì„¸ìš” (ì¤‘ë³µ ê¸ˆì§€!)
- "ì¿ í°" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ â†’ coupon_benefits
- "ì¿ í°" í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ â†’ benefits_by_purchase_amount

ì‘ë‹µ í˜•ì‹ (JSONë§Œ ì¶œë ¥):
{{
  "event_title": "ì œëª© ë˜ëŠ” null",
  "event_date": "ë‚ ì§œ ë˜ëŠ” null",
  "benefits_by_purchase_amount": ["í˜œíƒ1", "í˜œíƒ2"],
  "coupon_benefits": ["ì¿ í°1", "ì¿ í°2"]
}}

ì˜ˆì‹œ:
ì…ë ¥: "5ë§Œì› ì´ìƒ êµ¬ë§¤ì‹œ íŒŒìš°ì¹˜ ì¦ì •, 10% í• ì¸ ì¿ í° ì œê³µ"
ì¶œë ¥:
{{
  "benefits_by_purchase_amount": ["5ë§Œì› ì´ìƒ êµ¬ë§¤ì‹œ íŒŒìš°ì¹˜ ì¦ì •"],
  "coupon_benefits": ["10% í• ì¸ ì¿ í°"]
}}

ìµœì¢… í™•ì¸:
- ëª¨ë“  í˜œíƒì„ ë¹ ì§ì—†ì´ ì¶”ì¶œí–ˆë‚˜ìš”?
- ì¤‘ë³µëœ í•­ëª©ì´ ì—†ë‚˜ìš”?
- "ì¿ í°" í‚¤ì›Œë“œë¡œ ì •í™•íˆ ë¶„ë¥˜í–ˆë‚˜ìš”?
- JSON í˜•ì‹ë§Œ ì‘ë‹µí•˜ì„¸ìš” (ì„¤ëª… ì—†ì´)
"""

    def extract_event_info(self, ocr_text: str, url: str = None) -> Optional[Dict]:
        """
        Extract event information using LLM

        Args:
            ocr_text: OCR text from event images
            url: Event URL (for reference)

        Returns:
            Dictionary with extracted event information, or None if failed
        """
        prompt = self._build_prompt(ocr_text)

        try:
            if self.provider == LLMProvider.CLAUDE_HAIKU:
                return self._extract_claude(prompt)
            elif self.provider == LLMProvider.GPT_4O_MINI:
                return self._extract_openai(prompt)
            elif self.provider == LLMProvider.GEMINI_FLASH:
                return self._extract_gemini(prompt)
        except Exception as e:
            print(f"âš  LLM extraction failed ({self.provider.value}): {e}")
            return None

    def _extract_claude(self, prompt: str) -> Optional[Dict]:
        """Extract using Claude API"""
        response = self.client.messages.create(
            model=self.model,
            max_tokens=1024,
            messages=[{
                "role": "user",
                "content": prompt
            }]
        )

        result_text = response.content[0].text
        return self._parse_json_response(result_text)

    def _extract_openai(self, prompt: str) -> Optional[Dict]:
        """Extract using OpenAI API"""
        response = self.client.chat.completions.create(
            model=self.model,
            response_format={"type": "json_object"},
            messages=[{
                "role": "user",
                "content": prompt
            }]
        )

        result_text = response.choices[0].message.content
        return self._parse_json_response(result_text)

    def _extract_gemini(self, prompt: str) -> Optional[Dict]:
        """Extract using Google Gemini API"""
        response = self.client.generate_content(
            prompt,
            generation_config=genai.GenerationConfig(
                response_mime_type="application/json",
            )
        )

        result_text = response.text
        return self._parse_json_response(result_text)

    def _parse_json_response(self, text: str) -> Optional[Dict]:
        """Parse JSON from LLM response"""
        try:
            # Try direct JSON parsing
            return json.loads(text)
        except json.JSONDecodeError:
            # Try to extract JSON from markdown code blocks
            if "```json" in text:
                json_text = text.split("```json")[1].split("```")[0].strip()
            elif "```" in text:
                json_text = text.split("```")[1].split("```")[0].strip()
            else:
                # Try to find JSON object
                match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text, re.DOTALL)
                if match:
                    json_text = match.group(0)
                else:
                    print(f"âš  Could not find JSON in response: {text[:200]}")
                    return None

            try:
                return json.loads(json_text)
            except json.JSONDecodeError as e:
                print(f"âš  JSON parsing failed: {e}")
                print(f"   Text: {json_text[:200]}")
                return None

    def estimate_cost(self, ocr_text: str) -> Dict[str, float]:
        """
        Estimate API cost for extraction

        Args:
            ocr_text: OCR text to process

        Returns:
            Dictionary with cost breakdown
        """
        # Rough token estimation (1 token â‰ˆ 4 characters for English, 1-2 for Korean)
        input_tokens = len(ocr_text) / 2  # Conservative estimate for Korean
        prompt_tokens = 500  # Approximate prompt overhead
        total_input = input_tokens + prompt_tokens

        output_tokens = 200  # Expected JSON output

        input_cost_per_mtok, output_cost_per_mtok = self.COSTS[self.provider]

        input_cost = (total_input / 1_000_000) * input_cost_per_mtok
        output_cost = (output_tokens / 1_000_000) * output_cost_per_mtok
        total_cost = input_cost + output_cost

        return {
            "input_tokens": int(total_input),
            "output_tokens": output_tokens,
            "input_cost": input_cost,
            "output_cost": output_cost,
            "total_cost": total_cost,
            "provider": self.provider.value,
            "model": self.model
        }


def test_llm_extractor():
    """Test LLM extractor with sample data"""

    sample_text = """
ë·°í‹° í¬ì¸íŠ¸ ì ë¦½ ì´ë²¤íŠ¸
IOPE
2024.01.01 - 2024.01.31

ì—°ë™ í›„ êµ¬ë§¤ ì‹œ N pay 1% ì ë¦½
ì—°ë™ í›„ ìƒì‹œ êµ¬ë§¤ ì‹œ ë·°í‹°í¬ì¸íŠ¸ 1% ì ë¦½

5ë§Œì› ì´ìƒ êµ¬ë§¤ ì‹œ ì‚¬ì€í’ˆ ì¦ì •
10ë§Œì› ì´ìƒ êµ¬ë§¤ ì‹œ ì¶”ê°€ í• ì¸

4ì²œì› í• ì¸ ì¿ í°
ë¬´ë£Œë°°ì†¡ ì¿ í°

XMD ìŠ¤í…œ 3 ì„¸ëŸ¼
59,800ì› 37,310ì›
    """

    print("=" * 70)
    print("Testing LLM Extractors")
    print("=" * 70)

    # Test available providers
    providers_to_test = []

    if ANTHROPIC_AVAILABLE and os.environ.get("ANTHROPIC_API_KEY"):
        providers_to_test.append(LLMProvider.CLAUDE_HAIKU)

    if OPENAI_AVAILABLE and os.environ.get("OPENAI_API_KEY"):
        providers_to_test.append(LLMProvider.GPT_4O_MINI)

    if GOOGLE_AVAILABLE and os.environ.get("GOOGLE_API_KEY"):
        providers_to_test.append(LLMProvider.GEMINI_FLASH)

    if not providers_to_test:
        print("âŒ No LLM providers available")
        print("   Set API keys: ANTHROPIC_API_KEY, OPENAI_API_KEY, or GOOGLE_API_KEY")
        return

    for provider in providers_to_test:
        print(f"\n{'=' * 70}")
        print(f"Testing: {provider.value}")
        print(f"{'=' * 70}\n")

        try:
            extractor = LLMExtractor(provider=provider)

            # Estimate cost
            cost = extractor.estimate_cost(sample_text)
            print(f"ğŸ’° Estimated cost: ${cost['total_cost']:.6f}")
            print(f"   Input: {cost['input_tokens']} tokens (${cost['input_cost']:.6f})")
            print(f"   Output: {cost['output_tokens']} tokens (${cost['output_cost']:.6f})")

            # Extract
            print("\nğŸ”„ Extracting...")
            result = extractor.extract_event_info(sample_text)

            if result:
                print("âœ“ Extraction successful\n")
                print(json.dumps(result, ensure_ascii=False, indent=2))

                # Validate
                print("\nğŸ“Š Validation:")
                print(f"   Title: {'âœ“' if result.get('event_title') else 'âœ—'}")
                print(f"   Date: {'âœ“' if result.get('event_date') else 'âœ—'}")
                print(f"   Benefits: {len(result.get('benefits_by_purchase_amount', []))} found")
                print(f"   Coupons: {len(result.get('coupon_benefits', []))} found")
            else:
                print("âœ— Extraction failed")

        except Exception as e:
            print(f"âŒ Error: {e}")

    print(f"\n{'=' * 70}")
    print("âœ“ Testing complete")
    print(f"{'=' * 70}")


if __name__ == "__main__":
    test_llm_extractor()
