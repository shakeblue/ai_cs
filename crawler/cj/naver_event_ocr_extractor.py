#!/usr/bin/env python3
"""
Naver Event OCR Data Extractor
Extracts event information from images using OCR.space API
"""

import requests
import os
import json
import re
import time
import random
from pathlib import Path
from typing import Dict, List, Optional

# Import extraction strategies
try:
    from .extraction_strategy import ExtractionStrategy
    from .semantic_extractor import SemanticExtractor, SEMANTIC_AVAILABLE
    from .llm_extractor import LLMExtractor, LLMProvider
except ImportError:
    try:
        from extraction_strategy import ExtractionStrategy
        from semantic_extractor import SemanticExtractor, SEMANTIC_AVAILABLE
        from llm_extractor import LLMExtractor, LLMProvider
    except ImportError:
        SEMANTIC_AVAILABLE = False
        SemanticExtractor = None
        LLMExtractor = None
        ExtractionStrategy = None
        LLMProvider = None


class NaverEventOCRExtractor:
    """Extract event information from images using OCR"""

    def __init__(self, api_key: str = "K87899142388957", delay_min=1, delay_max=3,
                 strategy: 'ExtractionStrategy' = None, llm_provider: 'LLMProvider' = None):
        """
        Initialize OCR extractor

        Args:
            api_key: OCR.space API key
            delay_min: Minimum delay between API calls (seconds)
            delay_max: Maximum delay between API calls (seconds)
            strategy: Extraction strategy to use (default: AUTO)
            llm_provider: LLM provider if using LLM strategy (default: GPT4O_MINI)
        """
        self.api_key = api_key
        self.ocr_url = "https://api.ocr.space/parse/image"
        self.delay_min = delay_min
        self.delay_max = delay_max

        # Set default strategy
        if strategy is None:
            if ExtractionStrategy:
                strategy = ExtractionStrategy.AUTO
            else:
                strategy = "pattern"  # Fallback if module not available

        self.strategy = strategy
        self.llm_provider = llm_provider

        # Initialize extractors
        self.semantic_extractor = None
        self.llm_extractor = None

        # Initialize based on strategy
        self._initialize_extractors()

    def _initialize_extractors(self):
        """Initialize extractors based on selected strategy"""
        strategy_str = self.strategy.value if hasattr(self.strategy, 'value') else str(self.strategy)

        print(f"ğŸ”§ Initializing extraction strategy: {strategy_str}")

        # Determine which extractors are needed
        needs_semantic = any(s in strategy_str for s in ['semantic', 'auto'])
        needs_llm = any(s in strategy_str for s in ['llm', 'auto'])

        # Initialize semantic extractor if needed
        if needs_semantic and SEMANTIC_AVAILABLE:
            try:
                print("  ğŸ”„ Loading semantic extractor...")
                self.semantic_extractor = SemanticExtractor()
                print("  âœ“ Semantic extractor ready")
            except Exception as e:
                print(f"  âš  Semantic extractor failed: {e}")
                self.semantic_extractor = None

        # Initialize LLM extractor if needed
        if needs_llm and LLMExtractor:
            try:
                # Determine LLM provider
                if 'claude' in strategy_str:
                    provider = LLMProvider.CLAUDE_HAIKU
                elif 'gemini' in strategy_str:
                    provider = LLMProvider.GEMINI_FLASH
                elif 'gpt' in strategy_str:
                    provider = LLMProvider.GPT_4O_MINI
                elif self.llm_provider:
                    provider = self.llm_provider
                else:
                    provider = LLMProvider.GPT_4O_MINI  # Default

                print(f"  ğŸ”„ Loading LLM extractor ({provider.value})...")
                self.llm_extractor = LLMExtractor(provider=provider)
                print("  âœ“ LLM extractor ready")
            except Exception as e:
                print(f"  âš  LLM extractor failed: {e}")
                self.llm_extractor = None

        print(f"âœ“ Strategy initialized: {strategy_str}")

    def _delay(self, message="OCR API rate limiting"):
        """Add delay to respect API rate limits"""
        delay = random.uniform(self.delay_min, self.delay_max)
        print(f"    â± {message}: {delay:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
        time.sleep(delay)

    def extract_text_from_image(self, image_path: str) -> Optional[str]:
        """Extract text from image using OCR.space API"""
        try:
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

            with open(image_path, 'rb') as f:
                files = {'file': f}
                data = {
                    'apikey': self.api_key,
                    'language': 'kor',
                    'isOverlayRequired': False,
                    'detectOrientation': True,
                    'scale': True,
                    'OCREngine': 2  # Engine 2 for Asian languages
                }

                response = requests.post(
                    self.ocr_url,
                    files=files,
                    data=data,
                    timeout=30,
                    verify=False
                )

                if response.status_code == 200:
                    result = response.json()
                    if result.get('ParsedResults'):
                        text = result['ParsedResults'][0].get('ParsedText', '')
                        return text.strip()

            return None

        except Exception as e:
            print(f"    âœ— OCR ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def extract_all_text(self, image_files: List[str]) -> Dict[str, str]:
        """Extract text from all images with rate limiting"""
        print(f"\n{'='*70}")
        print("OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘")
        print(f"{'='*70}\n")

        extracted_texts = {}

        for idx, image_file in enumerate(image_files, 1):
            # Add delay between API calls (except first one)
            if idx > 1:
                self._delay(f"ì´ë¯¸ì§€ {idx} OCR ì „ ëŒ€ê¸°")

            print(f"[{idx}/{len(image_files)}] {os.path.basename(image_file)} ì²˜ë¦¬ ì¤‘...")

            text = self.extract_text_from_image(image_file)
            extracted_texts[image_file] = text or ""

            if text:
                print(f"    âœ“ {len(text)} ë¬¸ì ì¶”ì¶œ")
            else:
                print(f"    âœ— í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")

        print(f"\n{'='*70}")
        print(f"âœ“ OCR ì™„ë£Œ: {len([t for t in extracted_texts.values() if t])}/{len(image_files)}ê°œ ì„±ê³µ")
        print(f"{'='*70}\n")

        return extracted_texts

    def parse_event_info(self, texts: Dict[str, str], url: str, brand_name: str = None) -> Dict:
        """Parse event information using selected strategy"""
        print(f"\n{'='*70}")
        print("ì´ë²¤íŠ¸ ì •ë³´ íŒŒì‹±")
        print(f"{'='*70}\n")

        # Combine all text
        all_text = '\n'.join(texts.values())

        strategy_str = self.strategy.value if hasattr(self.strategy, 'value') else str(self.strategy)
        print(f"ğŸ“‹ Strategy: {strategy_str}")

        # Route to appropriate extraction method
        if strategy_str == 'pattern':
            event_data = self._extract_with_patterns(all_text, url, brand_name)

        elif strategy_str == 'semantic':
            event_data = self._extract_with_semantic(all_text, url, brand_name)

        elif 'llm-' in strategy_str:
            event_data = self._extract_with_llm(all_text, url, brand_name)

        elif strategy_str == 'hybrid-semantic-pattern':
            event_data = self._extract_hybrid_semantic_pattern(all_text, url, brand_name)

        elif strategy_str == 'hybrid-llm-pattern':
            event_data = self._extract_hybrid_llm_pattern(all_text, url, brand_name)

        elif strategy_str == 'hybrid-semantic-llm-pattern':
            event_data = self._extract_hybrid_semantic_llm_pattern(all_text, url, brand_name)

        elif strategy_str == 'auto':
            event_data = self._extract_auto(all_text, url, brand_name)

        else:
            print(f"âš  Unknown strategy '{strategy_str}', using pattern-based")
            event_data = self._extract_with_patterns(all_text, url, brand_name)

        # Print results
        print(f"\nâœ“ í”Œë«í¼: {event_data['platform_name']}")
        print(f"âœ“ ë¸Œëœë“œ: {event_data['brand_name']}")
        print(f"âœ“ ì´ë²¤íŠ¸ ì œëª©: {event_data['event_title']}")
        print(f"âœ“ ì´ë²¤íŠ¸ ê¸°ê°„: {event_data['event_date']}")
        print(f"âœ“ ê¸ˆì•¡ëŒ€ë³„ í˜œíƒ: {len(event_data['benefits_by_purchase_amount'])}ê°œ")
        print(f"âœ“ ì¿ í° í˜œíƒ: {len(event_data['coupon_benefits'])}ê°œ")

        return event_data

    def _extract_with_patterns(self, all_text: str, url: str, brand_name: str = None) -> Dict:
        """Extract event information using pattern-based approach"""
        print("ğŸ” Using pattern-based extraction...")
        return {
            'platform_name': 'Naver Brand',
            'brand_name': brand_name or self._extract_brand(all_text, url),
            'url': url,
            'event_title': self._extract_title(all_text),
            'event_date': self._extract_date(all_text),
            'benefits_by_purchase_amount': self._extract_purchase_benefits(all_text),
            'coupon_benefits': self._extract_coupon_benefits(all_text)
        }

    def _extract_with_semantic(self, all_text: str, url: str, brand_name: str = None) -> Dict:
        """Extract using semantic/NLP approach"""
        print("ğŸ” Using semantic extraction...")

        if not self.semantic_extractor:
            print("âš  Semantic extractor not available, falling back to patterns")
            return self._extract_with_patterns(all_text, url, brand_name)

        try:
            result = self.semantic_extractor.extract_all_semantic(all_text)
            return {
                'platform_name': 'Naver Brand',
                'brand_name': brand_name or self._extract_brand(all_text, url),
                'url': url,
                'event_title': result.get('title') or self._extract_title(all_text),
                'event_date': self._extract_date(all_text),
                'benefits_by_purchase_amount': result.get('benefits', []),
                'coupon_benefits': result.get('coupons', [])
            }
        except Exception as e:
            print(f"âš  Semantic extraction failed: {e}, falling back to patterns")
            return self._extract_with_patterns(all_text, url, brand_name)

    def _extract_with_llm(self, all_text: str, url: str, brand_name: str = None) -> Dict:
        """Extract using LLM approach"""
        print("ğŸ” Using LLM extraction...")

        if not self.llm_extractor:
            print("âš  LLM extractor not available, falling back to patterns")
            return self._extract_with_patterns(all_text, url, brand_name)

        try:
            result = self.llm_extractor.extract_event_info(all_text, url)
            if result:
                return {
                    'platform_name': 'Naver Brand',
                    'brand_name': brand_name or result.get('brand_name') or self._extract_brand(all_text, url),
                    'url': url,
                    'event_title': result.get('event_title'),
                    'event_date': result.get('event_date'),
                    'benefits_by_purchase_amount': result.get('benefits_by_purchase_amount', []),
                    'coupon_benefits': result.get('coupon_benefits', [])
                }
            else:
                print("âš  LLM returned no results, falling back to patterns")
                return self._extract_with_patterns(all_text, url, brand_name)
        except Exception as e:
            print(f"âš  LLM extraction failed: {e}, falling back to patterns")
            return self._extract_with_patterns(all_text, url, brand_name)

    def _extract_hybrid_semantic_pattern(self, all_text: str, url: str, brand_name: str = None) -> Dict:
        """Hybrid: Try semantic first, fallback to pattern"""
        print("ğŸ” Using hybrid extraction (Semantic â†’ Pattern)...")

        if self.semantic_extractor:
            try:
                result = self.semantic_extractor.extract_all_semantic(all_text)
                # Validate results
                if result.get('title') or result.get('benefits') or result.get('coupons'):
                    print("âœ“ Semantic successful")
                    return {
                        'platform_name': 'Naver Brand',
                        'brand_name': brand_name or self._extract_brand(all_text, url),
                        'url': url,
                        'event_title': result.get('title') or self._extract_title(all_text),
                        'event_date': self._extract_date(all_text),
                        'benefits_by_purchase_amount': result.get('benefits', []),
                        'coupon_benefits': result.get('coupons', [])
                    }
            except Exception as e:
                print(f"âš  Semantic failed: {e}")

        print("â†’ Falling back to pattern-based")
        return self._extract_with_patterns(all_text, url, brand_name)

    def _extract_hybrid_llm_pattern(self, all_text: str, url: str, brand_name: str = None) -> Dict:
        """Hybrid: Try LLM first, fallback to pattern"""
        print("ğŸ” Using hybrid extraction (LLM â†’ Pattern)...")

        if self.llm_extractor:
            try:
                result = self.llm_extractor.extract_event_info(all_text, url)
                if result:
                    print("âœ“ LLM successful")
                    return {
                        'platform_name': 'Naver Brand',
                        'brand_name': brand_name or result.get('brand_name') or self._extract_brand(all_text, url),
                        'url': url,
                        'event_title': result.get('event_title'),
                        'event_date': result.get('event_date'),
                        'benefits_by_purchase_amount': result.get('benefits_by_purchase_amount', []),
                        'coupon_benefits': result.get('coupon_benefits', [])
                    }
            except Exception as e:
                print(f"âš  LLM failed: {e}")

        print("â†’ Falling back to pattern-based")
        return self._extract_with_patterns(all_text, url, brand_name)

    def _extract_hybrid_semantic_llm_pattern(self, all_text: str, url: str, brand_name: str = None) -> Dict:
        """Hybrid: Try semantic â†’ LLM â†’ pattern"""
        print("ğŸ” Using hybrid extraction (Semantic â†’ LLM â†’ Pattern)...")

        # Try semantic first
        if self.semantic_extractor:
            try:
                result = self.semantic_extractor.extract_all_semantic(all_text)
                if result.get('title') or result.get('benefits') or result.get('coupons'):
                    print("âœ“ Semantic successful")
                    return {
                        'platform_name': 'Naver Brand',
                        'brand_name': brand_name or self._extract_brand(all_text, url),
                        'url': url,
                        'event_title': result.get('title') or self._extract_title(all_text),
                        'event_date': self._extract_date(all_text),
                        'benefits_by_purchase_amount': result.get('benefits', []),
                        'coupon_benefits': result.get('coupons', [])
                    }
            except Exception as e:
                print(f"âš  Semantic failed: {e}")

        # Try LLM if semantic failed
        print("â†’ Trying LLM...")
        if self.llm_extractor:
            try:
                result = self.llm_extractor.extract_event_info(all_text, url)
                if result:
                    print("âœ“ LLM successful")
                    return {
                        'platform_name': 'Naver Brand',
                        'brand_name': brand_name or result.get('brand_name') or self._extract_brand(all_text, url),
                        'url': url,
                        'event_title': result.get('event_title'),
                        'event_date': result.get('event_date'),
                        'benefits_by_purchase_amount': result.get('benefits_by_purchase_amount', []),
                        'coupon_benefits': result.get('coupon_benefits', [])
                    }
            except Exception as e:
                print(f"âš  LLM failed: {e}")

        # Final fallback to pattern
        print("â†’ Falling back to pattern-based")
        return self._extract_with_patterns(all_text, url, brand_name)

    def _extract_auto(self, all_text: str, url: str, brand_name: str = None) -> Dict:
        """Auto: Use best available strategy"""
        print("ğŸ” Using auto extraction (best available)...")

        # Prefer semantic if available
        if self.semantic_extractor:
            return self._extract_hybrid_semantic_llm_pattern(all_text, url, brand_name)
        # Try LLM if no semantic
        elif self.llm_extractor:
            return self._extract_hybrid_llm_pattern(all_text, url, brand_name)
        # Fallback to pattern
        else:
            return self._extract_with_patterns(all_text, url, brand_name)

    def _extract_brand(self, text: str, url: str = None) -> Optional[str]:
        """Extract brand with dynamic detection"""

        # 1. Try to extract from URL first
        if url:
            url_match = re.search(r'brand\.naver\.com/([^/]+)', url)
            if url_match:
                brand_from_url = url_match.group(1)
                # Validate it appears in text
                if re.search(brand_from_url, text, re.IGNORECASE):
                    return brand_from_url

        # 2. Known brands (extended list)
        known_brands = [
            'IOPE', 'ì•„ì´ì˜¤í˜', 'iope',
            'ì„¤í™”ìˆ˜', 'Sulwhasoo',
            'ë¼ë„¤ì¦ˆ', 'LANEIGE',
            'í—¤ë¼', 'HERA',
            'ì—ìŠ¤íŠ¸ë¼', 'AESTURA',
            'ì´ë‹ˆìŠ¤í”„ë¦¬', 'innisfree',
            # NEW: Add more brands
            'ë§ˆëª½ë“œ', 'MAMONDE',
            'ë ¤', 'RYO',
            'ë¯¸ìŸì„¼', 'MISE EN SCENE',
            'ì—ë›°ë“œ', 'ETUDE',
            'ì•„ëª¨ë ˆí¼ì‹œí”½', 'AMOREPACIFIC',
        ]

        for brand in known_brands:
            if re.search(brand, text, re.IGNORECASE):
                return brand.upper() if brand.isupper() else brand

        # 3. NEW: Look for brand-like patterns (capitalized words)
        brand_pattern = r'\b([A-Z][A-Za-z]{2,15})\b'
        matches = re.findall(brand_pattern, text)
        if matches:
            # Return most frequent capitalized word
            from collections import Counter
            counter = Counter(matches)
            most_common = counter.most_common(1)[0][0]
            if counter[most_common] >= 2:  # Appears at least twice
                return most_common

        return None

    def _extract_title(self, text: str) -> Optional[str]:
        """Extract event title with expanded patterns"""
        patterns = [
            # Original patterns
            r'([^\n]{5,50}ê¸°íšì „)',
            r'([^\n]{5,50}ì´ë²¤íŠ¸)',
            r'([^\n]{5,50}íŠ¹ê°€)',
            r'([^\n]{5,50}í”„ë¡œëª¨ì…˜)',

            # NEW: Additional event keywords
            r'([^\n]{5,50}ë”œ)',
            r'([^\n]{5,50}í• ì¸)',
            r'([^\n]{5,50}ì„¸ì¼)',
            r'([^\n]{5,50}ì‡¼í•‘)',
            r'([^\n]{5,50}ëŸ°ì¹­)',
            r'([^\n]{5,50}ì˜¤í”ˆ)',
            r'([^\n]{5,50}ë‹¨ë…)',
            r'([^\n]{5,50}í˜œíƒ)',
            r'([^\n]{5,50}ì ë¦½)',

            # NEW: Pattern for lines ending with exclamation/emphasis
            r'([^\n]{10,60}[!ï¼])',

            # NEW: Pattern for ALL CAPS titles
            r'([A-Z\s]{10,50})',

            # Keep product-specific as fallback
            r'(XMD[^\n]{0,40})',
            r'(ìŠ¤í…œ3[^\n]{0,40})',
        ]

        # Try patterns in order, return first valid match
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                title = match.group(1).strip()
                title = re.sub(r'\s+', ' ', title)
                if 5 <= len(title) <= 100:
                    return title

        # NEW: Fallback - use first substantial line
        lines = [l.strip() for l in text.split('\n') if l.strip()]
        for line in lines[:5]:  # Check first 5 lines
            if 10 <= len(line) <= 100 and not re.search(r'^\d+', line):
                return line

        return None

    def _extract_date(self, text: str) -> Optional[str]:
        """Extract event date from text"""
        # Date patterns
        patterns = [
            r'(\d{1,2}\.\d{1,2}\s*[~-]\s*\d{1,2}\.\d{1,2})',
            r'(\d{4}\.\d{1,2}\.\d{1,2}\s*[~-]\s*\d{4}\.\d{1,2}\.\d{1,2})',
            r'(\d{1,2}/\d{1,2}\s*[~-]\s*\d{1,2}/\d{1,2})',
        ]

        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1).strip()

        return None

    def _extract_purchase_benefits(self, text: str) -> List[str]:
        """Extract benefits with expanded patterns"""
        benefits = []
        lines = text.split('\n')

        # EXPANDED: Multiple benefit patterns
        benefit_patterns = [
            # Original
            r'(\d+ë§Œì›|ë§Œ\s*ì›)\s*ì´ìƒ\s*êµ¬ë§¤',

            # NEW: Additional amount patterns
            r'(\d+)ì²œì›\s*ì´ìƒ',
            r'(\d+)ì›\s*ì´ìƒ',
            r'(\d{1,3})(,\d{3})+ì›\s*ì´ìƒ',

            # NEW: Percentage patterns
            r'\d+%\s*(í• ì¸|ì ë¦½|ì¦ì •)',

            # NEW: Gift/benefit keywords
            r'êµ¬ë§¤\s*ì‹œ\s*.*?(ì¦ì •|ì œê³µ|ì ë¦½)',
            r'êµ¬ë§¤ê¸ˆì•¡ë³„',
            r'ê¸ˆì•¡ëŒ€ë³„',

            # NEW: Loyalty/points patterns
            r'(í¬ì¸íŠ¸|ì ë¦½)\s*\d+%',
            r'N\s*pay\s*\d+%',
            r'ë·°í‹°í¬ì¸íŠ¸.*?ì ë¦½',
        ]

        current_benefit = []

        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue

            # Check if line matches any benefit pattern
            is_benefit = False
            for pattern in benefit_patterns:
                if re.search(pattern, line):
                    is_benefit = True
                    break

            if is_benefit:
                if current_benefit:
                    benefits.append(' '.join(current_benefit))
                current_benefit = [line]

                # Look ahead for details (next 2 lines)
                for j in range(i+1, min(i+3, len(lines))):
                    next_line = lines[j].strip()
                    if next_line and not any(re.search(p, next_line) for p in benefit_patterns):
                        current_benefit.append(next_line)
                    else:
                        break

                if current_benefit:
                    benefits.append(' '.join(current_benefit))
                    current_benefit = []

        # Clean up
        benefits = [' '.join(b.split()) for b in benefits]
        benefits = list(dict.fromkeys(benefits))

        return benefits

    def _extract_coupon_benefits(self, text: str) -> List[str]:
        """Extract coupons with expanded patterns"""
        coupons = []
        lines = text.split('\n')
        processed_indices = set()  # Track processed lines to avoid duplicates

        # REFINED: More specific coupon patterns
        coupon_patterns = [
            # Strong coupon indicators (high priority)
            r'ì¿ í°',
            r'\d+%\s*(í• ì¸|OFF)',
            r'\d+ì²œ\s*ì›.*?(ì¿ í°|í• ì¸)',

            # Special benefits
            r'ë¬´ë£Œ\s*ë°°ì†¡',
            r'ë¬´ë£Œ\s*ì¦ì •',
            r'ì¶”ê°€\s*ì¦ì •',
        ]

        for i, line in enumerate(lines):
            if i in processed_indices:
                continue

            line = line.strip()
            if not line or len(line) < 5:  # Skip very short lines
                continue

            # Check if this line is a real coupon indicator
            is_coupon_line = False
            for pattern in coupon_patterns:
                if re.search(pattern, line):
                    is_coupon_line = True
                    break

            # Skip pure price displays (e.g., "59,800ì› 37,310ì›")
            if re.match(r'^[\d,]+ì›\s+[\d,]+ì›$', line):
                continue

            if is_coupon_line:
                coupon_info = [line]
                processed_indices.add(i)

                # Only add context if current line has strong coupon keyword
                if 'ì¿ í°' in line or 'í• ì¸' in line or 'OFF' in line:
                    # Context: previous line for coupon type (but avoid price-only lines)
                    if i > 0 and i-1 not in processed_indices:
                        prev_line = lines[i-1].strip()
                        if prev_line and len(prev_line) < 50:
                            # Skip if it's just prices
                            if not re.match(r'^[\d,]+ì›\s+[\d,]+ì›$', prev_line):
                                if not re.match(r'^\d+[\.,]\d+', prev_line):  # Skip number-only
                                    coupon_info.insert(0, prev_line)
                                    processed_indices.add(i-1)

                    # Context: next line for conditions (but avoid duplicating price displays)
                    if i + 1 < len(lines) and i+1 not in processed_indices:
                        next_line = lines[i+1].strip()
                        if next_line and len(next_line) < 100:
                            # Skip if it's just prices or already processed
                            if not re.match(r'^[\d,]+ì›\s+[\d,]+ì›$', next_line):
                                # Check if next line adds value
                                if 'êµ¬ë§¤' in next_line or 'ì´ìƒ' in next_line or 'ì¡°ê±´' in next_line:
                                    coupon_info.append(next_line)
                                    processed_indices.add(i+1)

                coupon_text = ' '.join(coupon_info)
                if coupon_text and len(coupon_text) > 5:
                    coupons.append(coupon_text)

        # Advanced deduplication: remove substrings
        unique_coupons = []
        for coupon in coupons:
            # Check if this coupon is a substring of any existing coupon
            is_substring = False
            for existing in unique_coupons:
                if coupon in existing or existing in coupon:
                    # Keep the longer one
                    if len(coupon) > len(existing):
                        unique_coupons.remove(existing)
                        unique_coupons.append(coupon)
                    is_substring = True
                    break

            if not is_substring:
                unique_coupons.append(coupon)

        # Clean up
        unique_coupons = [' '.join(c.split()) for c in unique_coupons]

        return unique_coupons

    def save_results(self, event_data: Dict, texts: Dict[str, str], output_file: str = 'naver_event_data.json'):
        """Save extraction results to JSON file"""
        result = {
            'event_data': event_data,
            'raw_ocr_texts': {os.path.basename(k): v for k, v in texts.items()}
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\nâœ“ ê²°ê³¼ ì €ì¥: {output_file}")

        return output_file


def main():
    """Test function"""
    # Test with downloaded images
    image_dir = 'naver_brand_images'

    if not os.path.exists(image_dir):
        print(f"âŒ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {image_dir}")
        return

    # Get all image files
    image_files = []
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp']:
        image_files.extend(Path(image_dir).glob(ext))

    image_files = [str(f) for f in image_files]

    if not image_files:
        print(f"âŒ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        return

    print(f"ğŸ“ ë°œê²¬ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ")

    # Extract OCR text
    extractor = NaverEventOCRExtractor()
    texts = extractor.extract_all_text(image_files)

    # Parse event info
    url = 'https://brand.naver.com/iope/shoppingstory/detail?id=5002337684'
    event_data = extractor.parse_event_info(texts, url, 'IOPE')

    # Save results
    extractor.save_results(event_data, texts)

    # Display results
    print(f"\n{'='*70}")
    print("ì¶”ì¶œëœ ì´ë²¤íŠ¸ ì •ë³´")
    print(f"{'='*70}\n")
    print(json.dumps(event_data, ensure_ascii=False, indent=2))


if __name__ == '__main__':
    main()
