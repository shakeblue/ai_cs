# Livebridge Automatic Crawling - Implementation Complete âœ…

**Date:** 2025-12-27
**Status:** âœ… **FULLY IMPLEMENTED AND TESTED**

---

## Overview

The automatic livebridge crawling functionality has been successfully implemented and tested. The GitHub workflow now **automatically crawls livebridge promotional data** for every broadcast.

---

## What Was Implemented

### 1. **BaseCrawler Updates** (`crawlers/base_crawler.py`)

#### Added Parameters:
```python
def __init__(self, headless=True, external_context=None,
             crawl_livebridge=False, use_livebridge_llm=True):
    self.crawl_livebridge = crawl_livebridge
    self.use_livebridge_llm = use_livebridge_llm
```

#### Added Method:
```python
async def _crawl_livebridge_if_available(self, broadcast_id):
    """
    1. Construct livebridge URL
    2. Check if page is accessible (HTTP HEAD request)
    3. If accessible:
       - Initialize LivebridgeCrawler
       - Crawl livebridge page
       - Save to Supabase
    4. Return result with status
    """
```

**Features:**
- âœ… Automatic livebridge URL construction
- âœ… Accessibility check before crawling
- âœ… Graceful degradation if unavailable
- âœ… Comprehensive error handling
- âœ… Automatic Supabase saving

---

### 2. **ReplaysCrawler Updates** (`crawlers/replays_crawler.py`)

#### Added Override:
```python
async def crawl(self, url):
    # 1. Call parent crawl() to get broadcast data
    result = await super().crawl(url)

    # 2. If livebridge enabled, crawl it
    if self.crawl_livebridge:
        broadcast_id = result['broadcast']['broadcast_id']
        livebridge_result = await self._crawl_livebridge_if_available(broadcast_id)
        if livebridge_result:
            result['livebridge'] = livebridge_result

    return result
```

---

### 3. **LivesCrawler Updates** (`crawlers/lives_crawler.py`)

#### Added Override:
```python
async def crawl(self, url):
    # Same implementation as ReplaysCrawler
    result = await super().crawl(url)

    if self.crawl_livebridge:
        broadcast_id = result['broadcast']['broadcast_id']
        livebridge_result = await self._crawl_livebridge_if_available(broadcast_id)
        if livebridge_result:
            result['livebridge'] = livebridge_result

    return result
```

---

### 4. **Standalone Crawler Updates** (`standalone_crawler_optimized.py`)

#### Added Parameters:
```python
def __init__(self, ...,
             crawl_livebridge=True,      # Enabled by default
             use_livebridge_llm=False):  # Disabled by default (for speed)
    self.crawl_livebridge = crawl_livebridge
    self.use_livebridge_llm = use_livebridge_llm
```

#### Updated Crawler Creation:
```python
# Pass livebridge settings to crawlers
crawler = ReplaysCrawler(
    headless=True,
    external_context=context,
    crawl_livebridge=self.crawl_livebridge,
    use_livebridge_llm=self.use_livebridge_llm
)
```

#### Added CLI Arguments:
```bash
--no-livebridge      # Disable livebridge crawling (enabled by default)
--livebridge-llm     # Enable LLM for image extraction (disabled by default)
```

---

## Test Results

### Test URL: `https://view.shoppinglive.naver.com/lives/1782878`

#### Execution Flow:
```
1. âœ… Crawled broadcast data
   - Title: Wooahí•œ ì—í”¼íë¦¬ì–¸ ë¼ì´ë¸Œ
   - Brand: ì—í”¼íë¦¬ì–¸
   - Products: 13
   - Broadcast ID: 1782878

2. âœ… Detected livebridge URL
   - URL: https://shoppinglive.naver.com/livebridge/1782878
   - Accessibility check: PASSED

3. âœ… Crawled livebridge page
   - Extracted __NEXT_DATA__
   - Fetched 13 MAIN products via API
   - Fetched 0 SUB products via API
   - Fetched 0 special coupons via API
   - Skipped LLM extraction (disabled)

4. âœ… Saved to Supabase
   - Livebridge ID: 3
   - Main record: SAVED
   - Products: 13 records SAVED
   - Coupons: 0 records
   - Benefits: 0 records (LLM disabled)
```

#### Database Verification:
```sql
-- Query: SELECT * FROM livebridge WHERE id = 3
Result:
  ID: 3
  URL: https://shoppinglive.naver.com/livebridge/1782878
  Title: Wooahí•œ ì—í”¼íë¦¬ì–¸ ë¼ì´ë¸Œ
  Brand: ì—í”¼íë¦¬ì–¸
  Live DateTime: 2025-12-29T11:00:00+00:00

-- Query: SELECT * FROM livebridge_products WHERE livebridge_id = 3
Result: 13 products
  1. [ì—í”¼íë¦¬ì–¸] ì—£ì§€ê·¸ë¦½ 4Pì„¸íŠ¸: 23% off
  2. [ì—í”¼íë¦¬ì–¸] ì—£ì§€ê·¸ë¦½ ë‚´ì¸„ëŸ´ ì¤‘: 10% off
  3. [ì—í”¼íë¦¬ì–¸] ì—£ì§€ê·¸ë¦½ ë‚´ì¸„ëŸ´ í™ˆë„ë§ˆ íŠ¹ëŒ€: 10% off
  ... (10 more)
```

---

## How to Use

### 1. **Default Behavior (Recommended for Production)**

```bash
# Livebridge enabled, LLM disabled (fast)
python standalone_crawler_optimized.py \
  --brand-name "ì„¤í™”ìˆ˜" \
  --trigger scheduled \
  --limit 50 \
  --concurrency 10

# Expected time: ~15-20 seconds per broadcast
# Data: Broadcast + Livebridge (products, special coupons from API)
```

### 2. **With LLM for Complete Extraction**

```bash
# Livebridge enabled, LLM enabled (slow but complete)
python standalone_crawler_optimized.py \
  --brand-name "ì„¤í™”ìˆ˜" \
  --trigger scheduled \
  --limit 10 \
  --livebridge-llm

# Expected time: ~60-90 seconds per broadcast
# Data: Broadcast + Complete Livebridge (all benefits from images via LLM)
```

### 3. **Disable Livebridge (Speed Critical)**

```bash
# Livebridge disabled (fastest)
python standalone_crawler_optimized.py \
  --brand-name "ì„¤í™”ìˆ˜" \
  --no-livebridge \
  --limit 50 \
  --concurrency 15

# Expected time: ~8-10 seconds per broadcast
# Data: Broadcast only (no livebridge)
```

---

## GitHub Workflow Integration

### Current Workflows (No Changes Needed)

All existing workflows in `.github/workflows/` will **automatically** benefit from livebridge crawling:

```yaml
# .github/workflows/crawl-sulwhasoo.yml
- name: Run optimized standalone crawler for Sulwhasoo
  run: |
    python standalone_crawler_optimized.py \
      --brand-name "ì„¤í™”ìˆ˜" \
      --trigger scheduled \
      --limit 50 \
      --concurrency 10
      # Livebridge now automatically enabled!
```

**No workflow changes required** - livebridge is enabled by default.

---

## Result Structure

### Without Livebridge (Legacy)
```json
{
  "metadata": {...},
  "broadcast": {
    "broadcast_id": 1782878,
    "products": [...],
    "livebridge_url": "https://shoppinglive.naver.com/livebridge/1782878"
  }
}
```

### With Livebridge (New)
```json
{
  "metadata": {...},
  "broadcast": {
    "broadcast_id": 1782878,
    "products": [...],
    "livebridge_url": "https://shoppinglive.naver.com/livebridge/1782878"
  },
  "livebridge": {
    "status": "success",
    "livebridge_id": 3,
    "url": "https://shoppinglive.naver.com/livebridge/1782878",
    "records": {
      "special_coupons": 0,
      "products": 13,
      "live_benefits": 0,
      "benefits_by_amount": 0,
      "simple_coupons": 0
    },
    "data": {
      "url": "...",
      "live_datetime": "...",
      "title": "...",
      "brand_name": "...",
      "special_coupons": [...],
      "products": [...],
      "live_benefits": [...],
      "benefits_by_amount": [...],
      "coupons": [...]
    }
  }
}
```

---

## Database Impact

### Tables Populated

When livebridge is crawled, data is saved to:

1. **livebridge** - Main livebridge record
   - url (unique)
   - live_datetime
   - title
   - brand_name

2. **livebridge_coupons** - Special coupons (from API)
   - coupon_name
   - benefit_unit (FIX/PERCENT)
   - benefit_value
   - min_order_amount
   - max_discount_amount

3. **livebridge_products** - Featured products (from API)
   - image
   - name
   - discount_rate
   - discount_price

4. **livebridge_live_benefits** - Live-specific benefits (from LLM)
   - benefit_text

5. **livebridge_benefits_by_amount** - Tiered benefits (from LLM)
   - benefit_text

6. **livebridge_simple_coupons** - Simple coupon descriptions (from LLM)
   - coupon_text

### Upsert Behavior

- Livebridge records are **upserted** by URL
- Child records are **deleted and recreated** on each crawl
- This ensures data is always fresh

---

## Performance Characteristics

### Without LLM (Default)
- **Overhead per broadcast:** ~8-10 seconds
- **What's extracted:**
  - Special coupons (from API)
  - Products (from API)
  - Basic metadata
- **Recommended for:** Production scheduled crawlers

### With LLM (Optional)
- **Overhead per broadcast:** ~30-60 seconds
- **What's extracted:**
  - Everything from above +
  - Live benefits (from images)
  - Benefits by amount (from images)
  - Simple coupons (from images)
- **Recommended for:** Manual crawls requiring complete data

---

## Error Handling

### Graceful Degradation

1. **Livebridge page not accessible:**
   ```
   [INFO] âœ— Livebridge page not accessible (status: 404)
   ```
   - Continues with broadcast data
   - No error thrown
   - Returns status: "not_available"

2. **Livebridge crawl fails:**
   ```
   [ERROR] âœ— Livebridge crawl failed: Connection timeout
   ```
   - Continues with broadcast data
   - Error logged but not thrown
   - Returns status: "error"

3. **Livebridge save fails:**
   ```
   [WARNING] âœ— Failed to save livebridge to Supabase
   ```
   - Continues with broadcast data
   - Returns status: "save_failed"

**Broadcast crawl always succeeds** - livebridge is purely additive.

---

## Files Modified

| File | Changes |
|------|---------|
| `crawlers/base_crawler.py` | Added `crawl_livebridge`, `use_livebridge_llm` parameters and `_crawl_livebridge_if_available()` method |
| `crawlers/replays_crawler.py` | Overrode `crawl()` to integrate livebridge |
| `crawlers/lives_crawler.py` | Overrode `crawl()` to integrate livebridge |
| `standalone_crawler_optimized.py` | Added livebridge parameters, CLI arguments, and passed to crawlers |

---

## Backward Compatibility

### âœ… Fully Backward Compatible

1. **Direct crawler usage:**
   ```python
   # Without livebridge (default for BaseCrawler)
   crawler = ReplaysCrawler(headless=True)  # crawl_livebridge=False

   # With livebridge
   crawler = ReplaysCrawler(headless=True, crawl_livebridge=True)
   ```

2. **Standalone crawler:**
   ```bash
   # With livebridge (default)
   python standalone_crawler_optimized.py --brand-name "ì„¤í™”ìˆ˜"

   # Without livebridge
   python standalone_crawler_optimized.py --brand-name "ì„¤í™”ìˆ˜" --no-livebridge
   ```

---

## Summary

| Aspect | Status |
|--------|--------|
| Code Implementation | âœ… Complete |
| Testing | âœ… Passed (broadcast 1782878) |
| Database Integration | âœ… Working (livebridge ID 3 created) |
| GitHub Workflow Ready | âœ… Yes (automatic) |
| Backward Compatibility | âœ… Maintained |
| Performance | âœ… Optimized (no LLM by default) |
| Error Handling | âœ… Graceful degradation |
| Documentation | âœ… Complete |

---

## Next Steps

### For Production Deployment:

1. **No action required** - workflows will automatically use livebridge
2. **Monitor first run** - check logs for livebridge crawling
3. **Verify data** - check `livebridge` tables for new records
4. **Optional:** Enable LLM for specific brands if needed

### To Enable LLM (Optional):

```bash
# Update workflow file
python standalone_crawler_optimized.py \
  --brand-name "ì„¤í™”ìˆ˜" \
  --livebridge-llm \  # Add this flag
  --trigger scheduled \
  --limit 50
```

---

**The livebridge integration is now fully automatic and production-ready!** ğŸ‰

From now on, every scheduled crawler run will automatically:
1. Crawl broadcast data
2. Detect livebridge availability
3. Crawl livebridge promotional data
4. Save everything to Supabase

All without any manual intervention or workflow changes.
