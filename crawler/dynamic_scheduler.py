#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë™ì  í”Œë«í¼ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬
ê´€ë¦¬ì ê¸°ëŠ¥ì—ì„œ ì¶”ê°€í•œ í”Œë«í¼ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  í¬ë¡¤ë§í•©ë‹ˆë‹¤.
"""

import schedule
import time
import logging
import json
import subprocess
import sys
from datetime import datetime
from pathlib import Path

# ë¡œê¹… ì„¤ì •
log_dir = Path(__file__).parent / 'logs'
log_dir.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(log_dir / f'dynamic_scheduler_{datetime.now().strftime("%Y%m%d")}.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# í”Œë«í¼ ì„¤ì • íŒŒì¼ ê²½ë¡œ
PLATFORMS_CONFIG_FILE = Path(__file__).parent / 'config' / 'platforms.json'


class DynamicDataCollectionScheduler:
    """ë™ì  ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”"""
        self.p_crawler_dir = Path(__file__).parent
        self.p_output_dir = self.p_crawler_dir / 'output'
        self.p_output_dir.mkdir(exist_ok=True)
        
        # í”Œë«í¼ë³„ í¬ë¡¤ëŸ¬ ë§¤í•‘
        self.p_platform_crawlers = {
            'NAVER': 'crawl_multi_brands.py',
            'KAKAO': 'kakao_live_crawler.py',
            '11ST': 'crawl_multi_brands.py',
            'GMARKET': 'crawl_multi_brands.py',
            'OLIVEYOUNG': 'crawl_multi_brands.py',
            'GRIP': 'crawl_multi_brands.py',
            'MUSINSA': 'crawl_multi_brands.py',
            'LOTTEON': 'crawl_multi_brands.py',
            'AMOREMALL': 'crawl_multi_brands.py',
            'INNISFREE_MALL': 'parsers/innisfree_live_parser.py',
            'NAVER_SHOPPING': 'naver_shopping_crawler.py',
        }
        
        # ìˆ˜ì§‘ í†µê³„
        self.p_stats = {
            'total_runs': 0,
            'successful_runs': 0,
            'failed_runs': 0,
            'last_run': None,
            'last_success': None,
            'last_error': None,
            'platforms_processed': {}
        }
    
    def load_platforms(self):
        """
        í”Œë«í¼ ì„¤ì • íŒŒì¼ ë¡œë“œ
        
        Returns:
            list: í™œì„±í™”ëœ í”Œë«í¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if not PLATFORMS_CONFIG_FILE.exists():
                logger.warning(f"âš ï¸ í”Œë«í¼ ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {PLATFORMS_CONFIG_FILE}")
                logger.info("ğŸ’¡ ê¸°ë³¸ í”Œë«í¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return self._get_default_platforms()
            
            with open(PLATFORMS_CONFIG_FILE, 'r', encoding='utf-8') as f:
                platforms = json.load(f)
            
            # í™œì„±í™”ëœ í”Œë«í¼ë§Œ í•„í„°ë§
            active_platforms = [p for p in platforms if p.get('isActive', True)]
            
            logger.info(f"ğŸ“¦ í”Œë«í¼ ì„¤ì • ë¡œë“œ ì™„ë£Œ: {len(active_platforms)}ê°œ í™œì„± í”Œë«í¼")
            return active_platforms
            
        except Exception as e:
            logger.error(f"âŒ í”Œë«í¼ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self._get_default_platforms()
    
    def _get_default_platforms(self):
        """ê¸°ë³¸ í”Œë«í¼ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        return [
            {'code': 'NAVER', 'name': 'ë„¤ì´ë²„', 'url': 'https://shoppinglive.naver.com', 'isActive': True},
            {'code': 'KAKAO', 'name': 'ì¹´ì¹´ì˜¤', 'url': 'https://shoppinglive.kakao.com', 'isActive': True},
        ]
    
    def collect_platform_data(self, platform):
        """
        íŠ¹ì • í”Œë«í¼ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            platform (dict): í”Œë«í¼ ì •ë³´
        """
        platform_code = platform.get('code', '')
        platform_name = platform.get('name', 'Unknown')
        platform_url = platform.get('url', '')
        
        logger.info("=" * 80)
        logger.info(f"ğŸ”„ {platform_name} ({platform_code}) ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        logger.info(f"   URL: {platform_url}")
        logger.info("=" * 80)
        
        try:
            # í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ ê²°ì •
            crawler_script = self.p_platform_crawlers.get(platform_code)
            
            if not crawler_script:
                # ê¸°ë³¸ í¬ë¡¤ëŸ¬ ì‚¬ìš© ë˜ëŠ” í”Œë«í¼ë³„ íŒŒì„œ ì‚¬ìš©
                if 'INNISFREE' in platform_code.upper() or 'innisfree' in platform_url.lower():
                    crawler_script = 'parsers/innisfree_live_parser.py'
                else:
                    crawler_script = 'crawl_multi_brands.py'
            
            crawler_path = self.p_crawler_dir / crawler_script
            
            if not crawler_path.exists():
                logger.warning(f"âš ï¸ í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤: {crawler_path}")
                logger.info(f"ğŸ’¡ ê¸°ë³¸ ë©€í‹° ë¸Œëœë“œ í¬ë¡¤ëŸ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                crawler_path = self.p_crawler_dir / 'crawl_multi_brands.py'
            
            if crawler_path.exists():
                logger.info(f"ğŸ“ ì‹¤í–‰: {crawler_path}")
                
                # í¬ë¡¤ëŸ¬ ì‹¤í–‰ (í”Œë«í¼ URLì„ ì¸ìë¡œ ì „ë‹¬)
                if crawler_script == 'parsers/innisfree_live_parser.py':
                    # ì´ë‹ˆìŠ¤í”„ë¦¬ ë¼ì´ë¸Œ íŒŒì„œëŠ” URLê³¼ ì½”ë“œë¥¼ ì¸ìë¡œ ë°›ìŒ
                    result = subprocess.run(
                        ['python3', str(crawler_path), platform_url, platform_code],
                        cwd=str(self.p_crawler_dir),
                        capture_output=True,
                        text=True,
                        timeout=600  # 10ë¶„ íƒ€ì„ì•„ì›ƒ
                    )
                else:
                    # ê¸°íƒ€ í¬ë¡¤ëŸ¬ëŠ” ê¸°ë³¸ ì‹¤í–‰
                    result = subprocess.run(
                        ['python3', str(crawler_path)],
                        cwd=str(self.p_crawler_dir),
                        capture_output=True,
                        text=True,
                        timeout=600
                    )
                
                if result.returncode == 0:
                    logger.info(f"âœ… {platform_name} ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ!")
                    logger.info(f"ì¶œë ¥:\n{result.stdout[:500]}")  # ì²˜ìŒ 500ìë§Œ ì¶œë ¥
                    self.p_stats['platforms_processed'][platform_code] = {
                        'status': 'success',
                        'last_success': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                else:
                    logger.error(f"âŒ {platform_name} í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨ (ì½”ë“œ: {result.returncode})")
                    logger.error(f"ì—ëŸ¬:\n{result.stderr[:500]}")
                    self.p_stats['platforms_processed'][platform_code] = {
                        'status': 'failed',
                        'last_error': result.stderr[:200]
                    }
            else:
                logger.error(f"âŒ í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {crawler_path}")
                self.p_stats['platforms_processed'][platform_code] = {
                    'status': 'failed',
                    'last_error': 'í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ ì—†ìŒ'
                }
                
        except subprocess.TimeoutExpired:
            logger.error(f"âŒ {platform_name} í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ (10ë¶„ ì´ˆê³¼)")
            self.p_stats['platforms_processed'][platform_code] = {
                'status': 'failed',
                'last_error': 'íƒ€ì„ì•„ì›ƒ'
            }
        except Exception as e:
            logger.error(f"âŒ {platform_name} ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            self.p_stats['platforms_processed'][platform_code] = {
                'status': 'failed',
                'last_error': str(e)[:200]
            }
    
    def collect_all_platforms_data(self):
        """ëª¨ë“  í™œì„± í”Œë«í¼ ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("=" * 80)
        logger.info("ğŸ”„ ì „ì²´ í”Œë«í¼ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        logger.info("=" * 80)
        
        try:
            self.p_stats['total_runs'] += 1
            self.p_stats['last_run'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # í”Œë«í¼ ì„¤ì • ë¡œë“œ
            platforms = self.load_platforms()
            
            if not platforms:
                logger.warning("âš ï¸ ìˆ˜ì§‘í•  í”Œë«í¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            logger.info(f"ğŸ“¦ ìˆ˜ì§‘ ëŒ€ìƒ í”Œë«í¼: {len(platforms)}ê°œ")
            
            success_count = 0
            fail_count = 0
            
            # ê° í”Œë«í¼ë³„ë¡œ ìˆ˜ì§‘
            for platform in platforms:
                try:
                    self.collect_platform_data(platform)
                    success_count += 1
                except Exception as e:
                    logger.error(f"âŒ í”Œë«í¼ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
                    fail_count += 1
                
                # í”Œë«í¼ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(5)
            
            self.p_stats['successful_runs'] += 1 if success_count > 0 else 0
            self.p_stats['failed_runs'] += fail_count
            
            if success_count > 0:
                self.p_stats['last_success'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
        except Exception as e:
            logger.error(f"âŒ ì „ì²´ í”Œë«í¼ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            self.p_stats['failed_runs'] += 1
            self.p_stats['last_error'] = str(e)[:500]
        finally:
            self._save_stats()
            logger.info("=" * 80)
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ ì™„ë£Œ | ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {fail_count}")
            logger.info("=" * 80)
    
    def _save_stats(self):
        """ìˆ˜ì§‘ í†µê³„ ì €ì¥"""
        try:
            stats_file = self.p_output_dir / 'dynamic_scheduler_stats.json'
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.p_stats, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"í†µê³„ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def get_stats(self):
        """ìˆ˜ì§‘ í†µê³„ ì¶œë ¥"""
        logger.info("=" * 80)
        logger.info("ğŸ“Š ë™ì  ë°ì´í„° ìˆ˜ì§‘ í†µê³„")
        logger.info("=" * 80)
        logger.info(f"ì´ ì‹¤í–‰ íšŸìˆ˜: {self.p_stats['total_runs']}")
        logger.info(f"ì„±ê³µ: {self.p_stats['successful_runs']}")
        logger.info(f"ì‹¤íŒ¨: {self.p_stats['failed_runs']}")
        logger.info(f"ë§ˆì§€ë§‰ ì‹¤í–‰: {self.p_stats['last_run']}")
        logger.info(f"ë§ˆì§€ë§‰ ì„±ê³µ: {self.p_stats['last_success']}")
        
        if self.p_stats['platforms_processed']:
            logger.info("\ní”Œë«í¼ë³„ í†µê³„:")
            for code, stats in self.p_stats['platforms_processed'].items():
                logger.info(f"  - {code}: {stats.get('status', 'unknown')}")
        
        if self.p_stats['last_error']:
            logger.info(f"ë§ˆì§€ë§‰ ì—ëŸ¬: {self.p_stats['last_error'][:200]}")
        logger.info("=" * 80)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("=" * 80)
    logger.info("ğŸš€ ë™ì  í”Œë«í¼ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
    logger.info("=" * 80)
    logger.info("â° ìˆ˜ì§‘ ì£¼ê¸°: ë§¤ ì‹œê°„ ì •ê° (00:00, 01:00, 02:00, ..., 23:00)")
    logger.info("ğŸ“¦ ìˆ˜ì§‘ ëŒ€ìƒ: ê´€ë¦¬ì ê¸°ëŠ¥ì—ì„œ ì¶”ê°€í•œ í™œì„± í”Œë«í¼")
    logger.info("=" * 80)
    
    scheduler = DynamicDataCollectionScheduler()
    
    # ì‹œì‘ ì‹œ ì¦‰ì‹œ 1íšŒ ì‹¤í–‰
    logger.info("ğŸ”„ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰...")
    scheduler.collect_all_platforms_data()
    
    # ë§¤ ì‹œê°„ ì •ê°ì— ì‹¤í–‰ë˜ë„ë¡ ìŠ¤ì¼€ì¤„ ë“±ë¡
    # schedule.every().hour.at(":00")ëŠ” ë§¤ ì‹œê°„ 00ë¶„ì— ì‹¤í–‰ë¨ (ì˜ˆ: 09:00, 10:00, 11:00, ...)
    schedule.every().hour.at(":00").do(scheduler.collect_all_platforms_data)
    
    # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ ê³„ì‚° ë° ë¡œê¹…
    _v_now = datetime.now()
    _v_next_hour = _v_now.replace(minute=0, second=0, microsecond=0)
    if _v_next_hour <= _v_now:
        _v_next_hour = _v_next_hour.replace(hour=_v_next_hour.hour + 1)
    _v_next_run_str = _v_next_hour.strftime("%Y-%m-%d %H:%M:%S")
    
    logger.info(f"â° ë‹¤ìŒ ìˆ˜ì§‘ ì˜ˆì • ì‹œê°„: {_v_next_run_str}")
    logger.info("=" * 80)
    
    # ë§¤ì¼ ìì •ì— í†µê³„ ì¶œë ¥
    schedule.every().day.at("00:00").do(scheduler.get_stats)
    
    logger.info("âœ… ìŠ¤ì¼€ì¤„ëŸ¬ ì¤€ë¹„ ì™„ë£Œ")
    logger.info("ğŸ’¡ Ctrl+Cë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    logger.info("=" * 80)
    
    try:
        while True:
            # ìŠ¤ì¼€ì¤„ ì‹¤í–‰
            schedule.run_pending()
            
            # ë‹¤ìŒ ì‹¤í–‰ ì‹œê°„ í™•ì¸ ë° ë¡œê¹… (ë§¤ 10ë¶„ë§ˆë‹¤)
            _v_current_time = datetime.now()
            if _v_current_time.minute % 10 == 0 and _v_current_time.second < 5:
                _v_next_jobs = schedule.jobs
                if _v_next_jobs:
                    _v_next_job = _v_next_jobs[0]
                    logger.info(f"â° ë‹¤ìŒ ìˆ˜ì§‘ ì˜ˆì • ì‹œê°„: {_v_next_job.next_run.strftime('%Y-%m-%d %H:%M:%S')}")
            
            time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ìŠ¤ì¼€ì¤„ ì²´í¬
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸  ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ")
        scheduler.get_stats()


if __name__ == "__main__":
    main()

