#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ë„¤ì´ë²„ ì‡¼í•‘ ì „ì‹œ í˜ì´ì§€ í¬ë¡¤ëŸ¬
ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ì‡¼í•‘ìŠ¤í† ë¦¬ ì „ì‹œ í˜ì´ì§€ì—ì„œ í–‰ì‚¬ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

ìˆ˜ì§‘ í•­ëª©:
1. í”Œë«í¼ëª…: ë„¤ì´ë²„ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´
2. ë¸Œëœë“œëª…
3. í–‰ì‚¬ íƒ€ì´í‹€
4. í–‰ì‚¬ ì¼ì
5. í˜œíƒ ì •ë³´ (ê¸ˆì•¡ëŒ€ë³„ í˜œíƒ, ì¿ í°)
6. ìƒí’ˆ ì •ë³´
"""

import os
import sys
import time
import re
import json
from datetime import datetime
from typing import Dict, List, Optional
from urllib.parse import urlparse, parse_qs

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from supabase import create_client, Client

# ë¡œê¹… ì„¤ì •
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('naver_shopping_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class NaverShoppingCrawler:
    """ë„¤ì´ë²„ ì‡¼í•‘ ì „ì‹œ í˜ì´ì§€ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.driver = None
        self.supabase: Optional[Client] = None
        self._init_supabase()
    
    def _init_supabase(self):
        """Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            _v_url = os.getenv('SUPABASE_URL')
            _v_key = os.getenv('SUPABASE_ANON_KEY')
            
            if not _v_url or not _v_key:
                logger.warning("âš ï¸ Supabase í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            
            self.supabase = create_client(_v_url, _v_key)
            logger.info("âœ… Supabase ì—°ê²° ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
    
    def _init_driver(self):
        """Selenium WebDriver ì´ˆê¸°í™”"""
        try:
            _v_chrome_options = Options()
            _v_chrome_options.add_argument('--headless')
            _v_chrome_options.add_argument('--no-sandbox')
            _v_chrome_options.add_argument('--disable-dev-shm-usage')
            _v_chrome_options.add_argument('--disable-gpu')
            _v_chrome_options.add_argument('--window-size=1920,1080')
            _v_chrome_options.add_argument('--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            self.driver = webdriver.Chrome(
                service=Service(ChromeDriverManager().install()),
                options=_v_chrome_options
            )
            logger.info("âœ… WebDriver ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            logger.error(f"âŒ WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def crawl(self, p_url: str, p_brand: str = "ì•„ì´ì˜¤í˜") -> Dict:
        """
        ë„¤ì´ë²„ ì‡¼í•‘ ì „ì‹œ í˜ì´ì§€ í¬ë¡¤ë§
        
        Args:
            p_url: í¬ë¡¤ë§í•  URL
            p_brand: ë¸Œëœë“œëª…
        
        Returns:
            ìˆ˜ì§‘ëœ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ” ë„¤ì´ë²„ ì‡¼í•‘ ì „ì‹œ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘")
        logger.info(f"{'='*80}")
        logger.info(f"ğŸ“ URL: {p_url}")
        logger.info(f"ğŸ·ï¸  ë¸Œëœë“œ: {p_brand}")
        
        try:
            # WebDriver ì´ˆê¸°í™”
            if not self.driver:
                self._init_driver()
            
            # í˜ì´ì§€ ë¡œë“œ
            logger.info("\nğŸ“„ í˜ì´ì§€ ë¡œë”© ì¤‘...")
            self.driver.get(p_url)
            time.sleep(5)  # ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
            
            # í˜ì´ì§€ ìŠ¤í¬ë¡¤ (lazy loading ì½˜í…ì¸  ë¡œë“œ)
            logger.info("ğŸ“œ í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì¤‘...")
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
            time.sleep(2)
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)
            
            # ë°ì´í„° ìˆ˜ì§‘
            _v_data = {
                'platform': 'ë„¤ì´ë²„ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´',
                'brand': p_brand,
                'url': p_url,
                'event_id': self._extract_event_id(p_url),
                'title': self._collect_title(),
                'date_info': self._collect_date(),
                'benefits': self._collect_benefits(),
                'coupons': self._collect_coupons(),
                'products': self._collect_products(),
                'collected_at': datetime.now().isoformat()
            }
            
            logger.info(f"\nâœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
            logger.info(f"   íƒ€ì´í‹€: {_v_data['title']}")
            logger.info(f"   í–‰ì‚¬ ì¼ì: {_v_data['date_info']}")
            logger.info(f"   í˜œíƒ: {len(_v_data['benefits'])}ê°œ")
            logger.info(f"   ì¿ í°: {len(_v_data['coupons'])}ê°œ")
            logger.info(f"   ìƒí’ˆ: {len(_v_data['products'])}ê°œ")
            
            return _v_data
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return {}
    
    def _extract_event_id(self, p_url: str) -> str:
        """URLì—ì„œ ì´ë²¤íŠ¸ ID ì¶”ì¶œ"""
        try:
            _v_parsed = urlparse(p_url)
            _v_params = parse_qs(_v_parsed.query)
            _v_event_id = _v_params.get('id', [''])[0]
            return f"NAVER_SHOPPING_{_v_event_id}" if _v_event_id else f"NAVER_SHOPPING_{int(time.time())}"
        except:
            return f"NAVER_SHOPPING_{int(time.time())}"
    
    def _collect_title(self) -> str:
        """í–‰ì‚¬ íƒ€ì´í‹€ ìˆ˜ì§‘"""
        logger.info("\nğŸ“Œ [1] í–‰ì‚¬ íƒ€ì´í‹€ ìˆ˜ì§‘ ì¤‘...")
        
        try:
            # í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ íƒ€ì´í‹€ ì°¾ê¸°
            _v_soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # ë°©ë²• 1: í˜ì´ì§€ íƒ€ì´í‹€
            _v_page_title = self.driver.title
            if _v_page_title and 'ì‡¼í•‘ìŠ¤í† ë¦¬' not in _v_page_title:
                logger.info(f"   âœ… íƒ€ì´í‹€ ë°œê²¬: {_v_page_title}")
                return _v_page_title.strip()
            
            # ë°©ë²• 2: í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ ë§¤ì¹­
            _v_all_text = _v_soup.get_text()
            
            # "ì•„ì´ì˜¤í˜ XMDìŠ¤í…œ3" ê°™ì€ íŒ¨í„´ ì°¾ê¸°
            _v_title_patterns = [
                r'ì•„ì´ì˜¤í˜\s+[A-Z]+[ê°€-í£\s]+',
                r'[ê°€-í£]+\s+ê¸°íšì „',
                r'[ê°€-í£]+\s+í”„ë¡œëª¨ì…˜',
                r'[ê°€-í£]+\s+ì´ë²¤íŠ¸'
            ]
            
            for pattern in _v_title_patterns:
                _v_match = re.search(pattern, _v_all_text)
                if _v_match:
                    _v_title = _v_match.group(0).strip()
                    logger.info(f"   âœ… íƒ€ì´í‹€ ë°œê²¬: {_v_title}")
                    return _v_title
            
            # ë°©ë²• 3: h1, h2 íƒœê·¸
            for tag in ['h1', 'h2', 'h3']:
                _v_heading = _v_soup.find(tag)
                if _v_heading:
                    _v_text = _v_heading.get_text(strip=True)
                    if _v_text and len(_v_text) > 5:
                        logger.info(f"   âœ… íƒ€ì´í‹€ ë°œê²¬: {_v_text}")
                        return _v_text
            
            logger.warning("   âš ï¸ íƒ€ì´í‹€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "ì œëª© ì—†ìŒ"
            
        except Exception as e:
            logger.error(f"   âŒ íƒ€ì´í‹€ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return "ì œëª© ì—†ìŒ"
    
    def _collect_date(self) -> str:
        """í–‰ì‚¬ ì¼ì ìˆ˜ì§‘"""
        logger.info("\nğŸ“… [2] í–‰ì‚¬ ì¼ì ìˆ˜ì§‘ ì¤‘...")
        
        try:
            _v_soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            _v_all_text = _v_soup.get_text()
            
            # ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
            _v_date_patterns = [
                r'\d{2,4}[./-]\d{1,2}[./-]\d{1,2}\s*~\s*\d{2,4}[./-]\d{1,2}[./-]\d{1,2}',  # ê¸°ê°„
                r'\d{2,4}[./-]\d{1,2}[./-]\d{1,2}',  # ë‹¨ì¼ ë‚ ì§œ
            ]
            
            for pattern in _v_date_patterns:
                _v_matches = re.findall(pattern, _v_all_text)
                if _v_matches:
                    _v_date = _v_matches[0]
                    logger.info(f"   âœ… í–‰ì‚¬ ì¼ì ë°œê²¬: {_v_date}")
                    return _v_date
            
            logger.warning("   âš ï¸ í–‰ì‚¬ ì¼ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            
        except Exception as e:
            logger.error(f"   âŒ í–‰ì‚¬ ì¼ì ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
    
    def _collect_benefits(self) -> List[Dict]:
        """í˜œíƒ ì •ë³´ ìˆ˜ì§‘ (ê¸ˆì•¡ëŒ€ë³„ í˜œíƒ)"""
        logger.info("\nğŸ [3] í˜œíƒ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        
        _v_benefits = []
        
        try:
            _v_soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            _v_all_text = _v_soup.get_text()
            
            # ê¸ˆì•¡ëŒ€ë³„ í˜œíƒ íŒ¨í„´
            _v_price_patterns = [
                r'(\d+ë§Œ?\s*ì›)\s*ì´ìƒ\s*êµ¬ë§¤\s*ì‹œ?\s*([^.]+)',
                r'ì „\s*êµ¬ë§¤\s*ê³ ê°\s*([^.]+)',
            ]
            
            for pattern in _v_price_patterns:
                _v_matches = re.findall(pattern, _v_all_text)
                for match in _v_matches:
                    if isinstance(match, tuple):
                        if len(match) == 2:
                            _v_benefits.append({
                                'type': 'ê¸ˆì•¡ëŒ€ë³„ í˜œíƒ',
                                'condition': match[0].strip(),
                                'benefit': match[1].strip()[:200]
                            })
                        else:
                            _v_benefits.append({
                                'type': 'ê¸ˆì•¡ëŒ€ë³„ í˜œíƒ',
                                'condition': 'ì „ êµ¬ë§¤ ê³ ê°',
                                'benefit': match[0].strip()[:200]
                            })
            
            logger.info(f"   âœ… í˜œíƒ {len(_v_benefits)}ê°œ ìˆ˜ì§‘")
            for idx, benefit in enumerate(_v_benefits, 1):
                logger.info(f"      [{idx}] {benefit['condition']}: {benefit['benefit'][:50]}")
            
            return _v_benefits
            
        except Exception as e:
            logger.error(f"   âŒ í˜œíƒ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _collect_coupons(self) -> List[Dict]:
        """ì¿ í° ì •ë³´ ìˆ˜ì§‘"""
        logger.info("\nğŸ« [4] ì¿ í° ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        
        _v_coupons = []
        
        try:
            _v_soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            _v_all_text = _v_soup.get_text()
            
            # ì¿ í° ê´€ë ¨ í…ìŠ¤íŠ¸ ì°¾ê¸°
            _v_coupon_keywords = ['ì¿ í°', 'COUPON']
            _v_lines = _v_all_text.split('\n')
            
            for line in _v_lines:
                for keyword in _v_coupon_keywords:
                    if keyword in line:
                        _v_clean_line = line.strip()
                        if _v_clean_line and len(_v_clean_line) > 3 and len(_v_clean_line) < 200:
                            _v_coupons.append({
                                'type': 'ì¿ í°',
                                'name': _v_clean_line
                            })
            
            # ì¤‘ë³µ ì œê±°
            _v_unique_coupons = []
            _v_seen = set()
            for coupon in _v_coupons:
                if coupon['name'] not in _v_seen:
                    _v_seen.add(coupon['name'])
                    _v_unique_coupons.append(coupon)
            
            logger.info(f"   âœ… ì¿ í° {len(_v_unique_coupons)}ê°œ ìˆ˜ì§‘")
            for idx, coupon in enumerate(_v_unique_coupons, 1):
                logger.info(f"      [{idx}] {coupon['name'][:80]}")
            
            return _v_unique_coupons
            
        except Exception as e:
            logger.error(f"   âŒ ì¿ í° ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _collect_products(self) -> List[Dict]:
        """ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘"""
        logger.info("\nğŸ›ï¸  [5] ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        
        _v_products = []
        
        try:
            # Seleniumìœ¼ë¡œ ìƒí’ˆ ìš”ì†Œ ì°¾ê¸°
            try:
                # ìƒí’ˆ ë§í¬ ì°¾ê¸° (a íƒœê·¸ ì¤‘ ìƒí’ˆ URL íŒ¨í„´)
                _v_product_links = self.driver.find_elements(By.CSS_SELECTOR, 'a[href*="/products/"]')
                
                logger.info(f"   ğŸ“¦ ìƒí’ˆ ë§í¬ {len(_v_product_links)}ê°œ ë°œê²¬")
                
                # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ set
                _v_seen_urls = set()
                
                for idx, link in enumerate(_v_product_links[:20], 1):  # ìµœëŒ€ 20ê°œ
                    try:
                        _v_url = link.get_attribute('href')
                        if not _v_url or _v_url in _v_seen_urls:
                            continue
                        
                        _v_seen_urls.add(_v_url)
                        
                        # ìƒí’ˆëª… ì°¾ê¸°
                        _v_name = link.text.strip()
                        if not _v_name or len(_v_name) < 3:
                            # ë¶€ëª¨ ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ ì°¾ê¸°
                            _v_parent = link.find_element(By.XPATH, '..')
                            _v_name = _v_parent.text.strip()
                        
                        if _v_name and len(_v_name) > 3:
                            _v_products.append({
                                'product_order': len(_v_products) + 1,
                                'product_name': _v_name[:200],
                                'product_url': _v_url,
                                'original_price': None,
                                'sale_price': None,
                                'discount_rate': None
                            })
                            
                            logger.info(f"      [{len(_v_products)}] {_v_name[:80]}")
                    
                    except Exception as e:
                        logger.debug(f"      ìƒí’ˆ {idx} íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
                
            except Exception as e:
                logger.warning(f"   âš ï¸ Seleniumìœ¼ë¡œ ìƒí’ˆ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            
            logger.info(f"   âœ… ìƒí’ˆ {len(_v_products)}ê°œ ìˆ˜ì§‘")
            return _v_products
            
        except Exception as e:
            logger.error(f"   âŒ ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def save_to_supabase(self, p_data: Dict) -> bool:
        """
        ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ Supabaseì— ì €ì¥
        
        Args:
            p_data: ìˆ˜ì§‘ëœ ë°ì´í„°
        
        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        if not self.supabase:
            logger.warning("âš ï¸ Supabase í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            logger.info(f"\n{'='*80}")
            logger.info("ğŸ’¾ Supabaseì— ë°ì´í„° ì €ì¥ ì¤‘...")
            logger.info(f"{'='*80}")
            
            # 1. live_broadcasts í…Œì´ë¸”ì— ì €ì¥
            _v_live_data = {
                'live_id': p_data['event_id'],
                'channel_code': 'NAVER_SHOPPING',
                'platform_name': p_data['platform'],
                'brand_name': p_data['brand'],
                'live_title_customer': p_data['title'],
                'live_title_cs': p_data['title'],
                'source_url': p_data['url'],
                'broadcast_date': datetime.now().date().isoformat(),
                'status': 'ACTIVE',  # ê¸°ë³¸ê°’
                'collected_at': datetime.now().isoformat()
            }
            
            # ê¸°ì¡´ ë°ì´í„° í™•ì¸
            _v_existing = self.supabase.table('live_broadcasts').select('*').eq('live_id', p_data['event_id']).execute()
            
            if _v_existing.data:
                # ì—…ë°ì´íŠ¸
                self.supabase.table('live_broadcasts').update(_v_live_data).eq('live_id', p_data['event_id']).execute()
                logger.info(f"   âœ… live_broadcasts ì—…ë°ì´íŠ¸: {p_data['event_id']}")
            else:
                # ì‚½ì…
                self.supabase.table('live_broadcasts').insert(_v_live_data).execute()
                logger.info(f"   âœ… live_broadcasts ì €ì¥: {p_data['event_id']}")
            
            # 2. ìƒí’ˆ ì •ë³´ ì €ì¥
            if p_data['products']:
                for product in p_data['products']:
                    _v_product_data = {
                        'live_id': p_data['event_id'],
                        'product_order': product.get('product_order', 0),
                        'product_name': product.get('product_name'),
                        'original_price': product.get('original_price'),
                        'sale_price': product.get('sale_price'),
                        'discount_rate': product.get('discount_rate'),
                        'product_url': product.get('product_url')
                    }
                    try:
                        self.supabase.table('live_products').insert(_v_product_data).execute()
                    except Exception as e:
                        logger.debug(f"   ìƒí’ˆ ì €ì¥ ì‹¤íŒ¨: {e}")
                
                logger.info(f"   âœ… ìƒí’ˆ {len(p_data['products'])}ê°œ ì €ì¥")
            
            # 3. í˜œíƒ/ì¿ í° ì •ë³´ëŠ” JSONìœ¼ë¡œ ì €ì¥ (ë©”íƒ€ë°ì´í„°)
            _v_metadata = {
                'benefits': p_data['benefits'],
                'coupons': p_data['coupons']
            }
            
            # live_broadcasts í…Œì´ë¸”ì— ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ (live_title_csì— ì €ì¥)
            _v_metadata_str = f"{p_data['title']} | í˜œíƒ: {len(p_data['benefits'])}ê°œ | ì¿ í°: {len(p_data['coupons'])}ê°œ"
            self.supabase.table('live_broadcasts').update({
                'live_title_cs': _v_metadata_str[:500]
            }).eq('live_id', p_data['event_id']).execute()
            
            logger.info(f"   âœ… í˜œíƒ/ì¿ í° ì •ë³´ ì €ì¥")
            logger.info(f"\n{'='*80}")
            logger.info("âœ… Supabase ì €ì¥ ì™„ë£Œ!")
            logger.info(f"{'='*80}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Supabase ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self.driver:
            self.driver.quit()
            logger.info("ğŸ”š WebDriver ì¢…ë£Œ")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ìƒ˜í”Œ URL
    _v_url = "https://brand.naver.com/iope/shoppingstory/detail?id=5002337684"
    _v_brand = "ì•„ì´ì˜¤í˜"
    
    _v_crawler = NaverShoppingCrawler()
    
    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        _v_data = _v_crawler.crawl(_v_url, _v_brand)
        
        if _v_data:
            # Supabaseì— ì €ì¥
            _v_crawler.save_to_supabase(_v_data)
            
            # JSON íŒŒì¼ë¡œë„ ì €ì¥
            _v_filename = f"naver_shopping_{_v_data['event_id']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(_v_filename, 'w', encoding='utf-8') as f:
                json.dump(_v_data, f, ensure_ascii=False, indent=2)
            logger.info(f"\nğŸ“ JSON íŒŒì¼ ì €ì¥: {_v_filename}")
    
    finally:
        _v_crawler.close()


if __name__ == "__main__":
    main()
